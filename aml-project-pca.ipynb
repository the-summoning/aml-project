{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-15T16:02:17.175578Z",
     "iopub.status.busy": "2025-12-15T16:02:17.174872Z",
     "iopub.status.idle": "2025-12-15T16:02:44.419492Z",
     "shell.execute_reply": "2025-12-15T16:02:44.418708Z",
     "shell.execute_reply.started": "2025-12-15T16:02:17.175547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from typing import Optional\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import os\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:03:10.702674Z",
     "iopub.status.busy": "2025-12-15T16:03:10.702165Z",
     "iopub.status.idle": "2025-12-15T16:03:24.498400Z",
     "shell.execute_reply": "2025-12-15T16:03:24.497575Z",
     "shell.execute_reply.started": "2025-12-15T16:03:10.702651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", # \"Manojb/stable-diffusion-2-1-base\" # \"CompVis/stable-diffusion-v1-4\",\n",
    "    safety_checker=None,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:03:29.987619Z",
     "iopub.status.busy": "2025-12-15T16:03:29.987341Z",
     "iopub.status.idle": "2025-12-15T16:03:29.997073Z",
     "shell.execute_reply": "2025-12-15T16:03:29.996483Z",
     "shell.execute_reply.started": "2025-12-15T16:03:29.987597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_unet_layers(pipe, extract_resnet=False, extract_attentions=True):\n",
    "    assert extract_resnet or extract_attentions\n",
    "    \n",
    "    nets = {}\n",
    "    \n",
    "    for i, block in enumerate(pipe.unet.down_blocks):\n",
    "        if extract_resnet:\n",
    "            for j, resnet in enumerate(block.resnets):\n",
    "               nets[f\"down_block_{i}_resnet_{j}\"] = resnet\n",
    "\n",
    "        if hasattr(block, \"attentions\") and extract_attentions:\n",
    "            for j, attn in enumerate(block.attentions):\n",
    "                for k, transformer in enumerate(attn.transformer_blocks):\n",
    "                    name = f\"down_block_{i}_attn_{j}_trans_{k}\"\n",
    "                    nets[name] = transformer.attn2\n",
    "\n",
    "    if extract_resnet:\n",
    "        for j, resnet in enumerate(pipe.unet.mid_block.resnets):\n",
    "            nets[f\"mid_block_resnet_{j}\"] = resnet\n",
    "\n",
    "    if hasattr(pipe.unet.mid_block, \"attentions\") and extract_attentions:\n",
    "        for j, attn in enumerate(pipe.unet.mid_block.attentions):\n",
    "            for k, transformer in enumerate(attn.transformer_blocks):\n",
    "                name = f\"mid_block_attn_{j}_trans_{k}\"\n",
    "                nets[name] = transformer.attn2\n",
    "                \n",
    "    \n",
    "    for i, block in enumerate(pipe.unet.up_blocks):\n",
    "        if extract_resnet:\n",
    "            for j, resnet in enumerate(block.resnets):\n",
    "                nets[f\"up_block_{i}_resnet_{j}\"] = resnet\n",
    "\n",
    "        if hasattr(block, \"attentions\") and extract_attentions:\n",
    "            for j, attn in enumerate(block.attentions):\n",
    "                for k, transformer in enumerate(attn.transformer_blocks):\n",
    "                    name = f\"up_block_{i}_attn_{j}_trans_{k}\"\n",
    "                    nets[name] = transformer.attn2\n",
    "\n",
    "    return nets\n",
    "\n",
    "\n",
    "nets = get_unet_layers(pipe, True, True)\n",
    "\n",
    "ALL_LAYERS = list(nets.keys())\n",
    "RESNET_LAYERS = [l for l in ALL_LAYERS if 'resnet_' in l]\n",
    "ATTENTION_LAYERS = [l for l in ALL_LAYERS if 'attn_' in l]\n",
    "\n",
    "print('Resnet layers:', RESNET_LAYERS, end='\\n\\n')\n",
    "print('Attention layers:', ATTENTION_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:03:32.842717Z",
     "iopub.status.busy": "2025-12-15T16:03:32.842078Z",
     "iopub.status.idle": "2025-12-15T16:03:37.405408Z",
     "shell.execute_reply": "2025-12-15T16:03:37.404625Z",
     "shell.execute_reply.started": "2025-12-15T16:03:32.842693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dogs_dataset = pd.read_csv('/kaggle/input/prompts-steering/dogs.csv')\n",
    "\n",
    "dog_prompts = dogs_dataset['positive'].tolist()\n",
    "non_dog_prompts = dogs_dataset['negative'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:52:13.404716Z",
     "iopub.status.busy": "2025-12-15T16:52:13.404174Z",
     "iopub.status.idle": "2025-12-15T16:52:13.418479Z",
     "shell.execute_reply": "2025-12-15T16:52:13.417568Z",
     "shell.execute_reply.started": "2025-12-15T16:52:13.404691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collect_average_activations(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    forget_set: list[str],\n",
    "    retain_set: list[str],\n",
    "    total_steps: int,\n",
    "    guidance: float,\n",
    "    nets: dict,\n",
    "    layers: list[str],\n",
    "    timesteps: list[int]\n",
    "):\n",
    "    forget_acts = []\n",
    "    retain_acts = []\n",
    "\n",
    "    for idx, (forget_prompt, retain_prompt) in enumerate(zip(forget_set, retain_set)):\n",
    "        print(f'[{idx+1}] Extracting acts for forget prompt: {forget_prompt}')\n",
    "        forget_act = get_average_activations(pipe, forget_prompt, total_steps, guidance, nets, layers, timesteps)\n",
    "\n",
    "        print(f'[{idx+1}] Extracting acts for retain prompt: {retain_prompt}')\n",
    "        retain_act = get_average_activations(pipe, retain_prompt, total_steps, guidance, nets, layers, timesteps)\n",
    "        \n",
    "        forget_acts.append(forget_act)\n",
    "        retain_acts.append(retain_act)\n",
    "\n",
    "    forget_layers = {}\n",
    "    retain_layers = {}\n",
    "    \n",
    "    for l in layers:\n",
    "        forget_layers[l] = torch.stack([f[l] for f in forget_acts], dim=0)\n",
    "        retain_layers[l] = torch.stack([r[l] for r in retain_acts], dim=0)\n",
    "        \n",
    "    return forget_layers, retain_layers\n",
    "\n",
    "\n",
    "\n",
    "def get_average_activations(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    prompt: str,\n",
    "    total_steps: int,\n",
    "    guidance: float,\n",
    "    nets: dict,\n",
    "    layers: list[str],\n",
    "    timesteps: list[int]\n",
    "):\n",
    "    # designed to be simple, using batches would cause coherence issues when collecting acts.\n",
    "    result = {}\n",
    "    handles = []\n",
    "\n",
    "    current_step = 0\n",
    "\n",
    "    def save_act(name):\n",
    "        def hook(module, input, output):           \n",
    "            if current_step in timesteps:\n",
    "                # UNet calculates noise prediction for both conditioned and unconditioned input, so we take the second\n",
    "                residual = output[1] if isinstance(output, tuple) else output\n",
    "\n",
    "                if residual[1].ndim == 3: # Channels x Width x Height\n",
    "                    act = residual[1].mean(dim=(1, 2)).detach().cpu()\n",
    "                elif residual[1].ndim == 2: # Tokens x Context \n",
    "                    act = residual[1].mean(dim=0).detach().cpu()\n",
    "                else:\n",
    "                    raise Exception(f'Unexpected activation shape {residual[1].shape} for {name}') \n",
    "                \n",
    "                result.setdefault(name, []).append(act)\n",
    "                \n",
    "        return hook\n",
    "\n",
    "    for l in layers:\n",
    "        handles.append(\n",
    "            nets[l].register_forward_hook(save_act(l))\n",
    "        )\n",
    "\n",
    "    def callback(pipeline, step_index, timestep, callback_kwargs):\n",
    "        nonlocal current_step\n",
    "        current_step = step_index\n",
    "\n",
    "        return callback_kwargs\n",
    "    \n",
    "    try:\n",
    "        images = pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=total_steps,\n",
    "            guidance_scale=guidance,\n",
    "            callback_on_step_end=callback\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            layer: torch.stack(tensors, dim=0)\n",
    "            for layer, tensors in result.items()\n",
    "        } # [T, C, H, W]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "def show_images(images: list[Image.Image], prompts: list[str], cols: int = 2, width: int = 40) -> None:\n",
    "    assert len(images) == len(prompts)\n",
    "\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax in axes[len(images):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax, img, prompt in zip(axes, images, prompts):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        wrapped_prompt = \"\\n\".join(textwrap.wrap(prompt, width=width))\n",
    "        ax.text(0.5, -0.05, wrapped_prompt, fontsize=10, ha='center', va='top', transform=ax.transAxes)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_color('black')\n",
    "            spine.set_linewidth(1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_timesteps_by_name(steps, name: str):\n",
    "    if name == 'f':\n",
    "        return list(range(0, steps // 2 + 1))\n",
    "    elif name == 'l':\n",
    "        return list(range(steps // 2, steps + 1))\n",
    "    elif name == 'all':\n",
    "        return list(range(0, steps + 1))\n",
    "    else:\n",
    "        raise ValueError(f'{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:52:15.284083Z",
     "iopub.status.busy": "2025-12-15T16:52:15.283574Z",
     "iopub.status.idle": "2025-12-15T16:52:15.287743Z",
     "shell.execute_reply": "2025-12-15T16:52:15.287134Z",
     "shell.execute_reply.started": "2025-12-15T16:52:15.284062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GUIDANCE = 7.5\n",
    "LAYERS = ATTENTION_LAYERS\n",
    "STEPS = 30\n",
    "TIMESTEPS_NAME = 'all'\n",
    "TIMESTEPS = get_timesteps_by_name(STEPS, TIMESTEPS_NAME)\n",
    "LAYER_NAV_K = len(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:03:43.059812Z",
     "iopub.status.busy": "2025-12-15T16:03:43.059325Z",
     "iopub.status.idle": "2025-12-15T16:08:59.120756Z",
     "shell.execute_reply": "2025-12-15T16:08:59.119972Z",
     "shell.execute_reply.started": "2025-12-15T16:03:43.059786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "forget_acts, retain_acts = collect_average_activations(\n",
    "    pipe,\n",
    "    dog_prompts,\n",
    "    non_dog_prompts,\n",
    "    total_steps=STEPS,\n",
    "    guidance=GUIDANCE,\n",
    "    nets=nets,\n",
    "    layers=LAYERS,\n",
    "    timesteps=TIMESTEPS\n",
    ")\n",
    "\n",
    "#for layer, act in forget_acts.items():\n",
    "#    print(f'Layer {layer}: {act.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:09:33.789343Z",
     "iopub.status.busy": "2025-12-15T16:09:33.789046Z",
     "iopub.status.idle": "2025-12-15T16:09:33.801659Z",
     "shell.execute_reply": "2025-12-15T16:09:33.801039Z",
     "shell.execute_reply.started": "2025-12-15T16:09:33.789321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_scores(retain_acts, forget_acts, timesteps, top_k):\n",
    "    results = {}\n",
    "\n",
    "    for timestep in timesteps :\n",
    "        timestep_dict = {}\n",
    "        for layer in retain_acts:\n",
    "            P = retain_acts[layer][:, timestep-timesteps[0], :].float()  # Positive (N, D)\n",
    "            N = forget_acts[layer][:, timestep-timesteps[0], :].float()  # Negative (N, D)\n",
    "    \n",
    "            if P.shape != N.shape:\n",
    "                print(f'P shape and N shape differs in {layer}')\n",
    "    \n",
    "            n_samples = P.shape[0]\n",
    "    \n",
    "            all_acts = torch.cat([P, N], dim=0) # (2N, D)\n",
    "            mu_l = all_acts.mean(dim=0, keepdim=True)  # (1, D)\n",
    "            sigma_l = all_acts.std(dim=0, keepdim=True) + 1e-8 # (1, D)\n",
    "    \n",
    "            P_tilde = (P - mu_l) / sigma_l\n",
    "            N_tilde = (N - mu_l) / sigma_l\n",
    "\n",
    "            v_l = (N - P).mean(dim=0)\n",
    "    \n",
    "            \n",
    "            # Calculate means of normalized data\n",
    "            mu_pos = P_tilde.mean(dim=0) # (D)\n",
    "            mu_neg = N_tilde.mean(dim=0) # (D)\n",
    "    \n",
    "            # Instead of creating (D, D) matrix, project means onto v_l\n",
    "            proj_pos = torch.dot(mu_pos, v_l)\n",
    "            proj_neg = torch.dot(mu_neg, v_l)\n",
    "            \n",
    "            # v^T Sb v = N * (proj_pos^2 + proj_neg^2)\n",
    "            sb_val = n_samples * (proj_pos**2 + proj_neg**2)\n",
    "    \n",
    "            # Center the data class-wise\n",
    "            P_centered = P_tilde - mu_pos.unsqueeze(0) # (N, D)\n",
    "            N_centered = N_tilde - mu_neg.unsqueeze(0) # (N, D)\n",
    "    \n",
    "            # Instead of creating (D, D) covariance, project data onto v_l\n",
    "            # This calculates the variance of the data along the direction of v_l\n",
    "            p_proj = torch.mv(P_centered, v_l) # (N)\n",
    "            n_proj = torch.mv(N_centered, v_l) # (N)\n",
    "    \n",
    "            sw_pos_val = torch.sum(p_proj**2)\n",
    "            sw_neg_val = torch.sum(n_proj**2)\n",
    "            sw_val = sw_pos_val + sw_neg_val\n",
    "    \n",
    "            \n",
    "            D_l = (sb_val / (sb_val + sw_val + 1e-8)).item()\n",
    "    \n",
    "            pair_diffs = N_tilde - P_tilde # (N, D)\n",
    "            dot_products = torch.mv(pair_diffs, v_l) # (N)\n",
    "            pair_norms = torch.norm(pair_diffs, dim=1) # (N,)\n",
    "            v_norm = torch.norm(v_l)\n",
    "            \n",
    "            cosine_sims = dot_products / (pair_norms * v_norm + 1e-8)\n",
    "            C_l = cosine_sims.mean().item()\n",
    "    \n",
    "            S_l = D_l + C_l\n",
    "    \n",
    "            timestep_dict[layer] = {\n",
    "                \"score\": S_l,\n",
    "                \"discriminability\": D_l,\n",
    "                \"consistency\": C_l\n",
    "            }\n",
    "            \n",
    "            del P_tilde, N_tilde, all_acts, P_centered, N_centered\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        sorted_layers = sorted(timestep_dict.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "        results[timestep] = [x for x in sorted_layers[:top_k]]\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_top_k_layers(results, k):\n",
    "    res = {}\n",
    "    for timestep, top in results.items():\n",
    "        res[timestep] = [x[0] for x in top[:k]]\n",
    "    return res\n",
    "\n",
    "def print_report(results):\n",
    "    for timestep, top in results.items():\n",
    "        print(f\"Timestep: {timestep}\")\n",
    "        for layer, score_dict in top:\n",
    "            score, disc, cons = score_dict['score'], score_dict['discriminability'], score_dict['consistency']\n",
    "            print(f'\\tLayer: {layer} | Score: {score} | Disc: {disc} | Cons: {cons}')\n",
    "\n",
    "def mask_vectors_by_top_k(steering_vectors, timesteps, top_k_per_timestep):\n",
    "    masked_vectors = {l: v.clone() for l, v in steering_vectors.items()}\n",
    "    \n",
    "    for ts_index, step in enumerate(timesteps):\n",
    "        active_layers = top_k_per_timestep.get(step, [])\n",
    "        \n",
    "        for layer_name, vector_tensor in masked_vectors.items():\n",
    "            # If this layer is NOT in the active list for this step, zero it out\n",
    "            if layer_name not in active_layers:\n",
    "                vector_tensor[ts_index] = 0.0\n",
    "\n",
    "    return masked_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:09:38.896216Z",
     "iopub.status.busy": "2025-12-15T16:09:38.895682Z",
     "iopub.status.idle": "2025-12-15T16:09:39.346522Z",
     "shell.execute_reply": "2025-12-15T16:09:39.345753Z",
     "shell.execute_reply.started": "2025-12-15T16:09:38.896192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = compute_scores(retain_acts, forget_acts, timesteps=TIMESTEPS, top_k=LAYER_NAV_K)\n",
    "\n",
    "#top_k_per_timestep = get_top_k_layers(results, k=LAYER_NAV_K)\n",
    "#print_report(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:09:41.526506Z",
     "iopub.status.busy": "2025-12-15T16:09:41.526242Z",
     "iopub.status.idle": "2025-12-15T16:09:41.531908Z",
     "shell.execute_reply": "2025-12-15T16:09:41.531270Z",
     "shell.execute_reply.started": "2025-12-15T16:09:41.526485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BEST_LAYERS = list(set([l[0] for value in results.values() for l in value]))\n",
    "\n",
    "#BEST_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def contrastive_pca(X, Y, alpha: float = 1.0, threshold: float = 0.95, min_eigen: float = 0.5):\n",
    "    X = X.float()\n",
    "    Y = Y.float()\n",
    "\n",
    "    X_center = X.mean(dim=0, keepdim=True)\n",
    "    Xc = X - X_center\n",
    "    Yc = Y - Y.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    N, D = Xc.shape\n",
    "    M, _ = Yc.shape\n",
    "\n",
    "    S_X = (Xc.T @ Xc) / (N - 1)\n",
    "    S_Y = (Yc.T @ Yc) / (M - 1)\n",
    "\n",
    "    S_c = S_X - alpha * S_Y\n",
    "\n",
    "    eigvals, eigvecs = torch.linalg.eigh(S_c)\n",
    "\n",
    "    eigvals, idx = torch.sort(eigvals, descending=True)\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "\n",
    "    mask = eigvals > min_eigen\n",
    "    eigvals_pos = eigvals[mask]\n",
    "    eigvecs_pos = eigvecs[:, mask]\n",
    "\n",
    "    cum_var = torch.cumsum(eigvals_pos, dim=0) / eigvals_pos.sum()\n",
    "    num_components = (cum_var < threshold).sum() + 1\n",
    "\n",
    "    eigvals_selected = eigvals_pos[:num_components]\n",
    "    eigvecs_selected = eigvecs_pos[:, :num_components]\n",
    "\n",
    "    return eigvecs_selected.cpu(), eigvals_selected.cpu(), X_center.cpu()\n",
    "\n",
    "\n",
    "def compute_principal_components(forget_acts, retain_acts, layers: list[str], min_subspace_dim: int = 5, alpha: float = 1.0, threshold: float = 0.95, min_eigen: float = 0.5):\n",
    "    result = defaultdict(lambda: defaultdict(int))\n",
    "    for (layer, X), (_, Y) in zip(forget_acts.items(), retain_acts.items()): \n",
    "        X = forget_acts[layer]\n",
    "        Y = retain_acts[layer]\n",
    "\n",
    "        assert X.shape == Y.shape\n",
    "\n",
    "        for ts in range(X.size(1)):\n",
    "            eigvecs, eigvals, mean = contrastive_pca(X[:, ts, :], Y[:, ts, :], alpha, threshold, min_eigen)\n",
    "\n",
    "            if len(eigvals) >= min_subspace_dim: # we require at least 5 PC  \n",
    "                #print(f'Layer {layer}: shape: {X.shape} ts: {ts}, eigvecs: {eigvecs.shape}, eigvals: {eigvals.tolist()}')\n",
    "                result[layer][ts] = (eigvecs, mean)\n",
    "\n",
    "    return result\n",
    "\n",
    "layer_principal_components = compute_principal_components(\n",
    "    forget_acts,\n",
    "    retain_acts,\n",
    "    layers=BEST_LAYERS,\n",
    "    min_subspace_dim=5,\n",
    "    alpha=10,\n",
    "    threshold=0.90,\n",
    "    min_eigen=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:10:15.107344Z",
     "iopub.status.busy": "2025-12-15T16:10:15.107067Z",
     "iopub.status.idle": "2025-12-15T16:10:15.112574Z",
     "shell.execute_reply": "2025-12-15T16:10:15.111824Z",
     "shell.execute_reply.started": "2025-12-15T16:10:15.107325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#layer_principal_components['down_block_2_attn_0_trans_0'][1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:10:50.942885Z",
     "iopub.status.busy": "2025-12-15T16:10:50.942085Z",
     "iopub.status.idle": "2025-12-15T16:10:50.967631Z",
     "shell.execute_reply": "2025-12-15T16:10:50.966974Z",
     "shell.execute_reply.started": "2025-12-15T16:10:50.942855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate steering vectors\n",
    "\n",
    "def compute_mean_differences(forget_layers_act, retain_layers_act):\n",
    "    result = {}\n",
    "    for (layer, X), (_, Y) in zip(forget_layers_act.items(), retain_layers_act.items()):\n",
    "        result[layer] = (X - Y).mean(dim=0)\n",
    "\n",
    "    return result\n",
    "\n",
    "steering_vectors = compute_mean_differences(forget_acts, retain_acts)\n",
    "\n",
    "#filtered_vectors = mask_vectors_by_top_k(\n",
    "#    steering_vectors, \n",
    "#    timesteps=TIMESTEPS, \n",
    "#    top_k_per_timestep=top_k_per_timestep\n",
    "#)\n",
    "#print([(layer, steering_vector.shape) for (layer, steering_vector) in steering_vectors.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:44:22.722518Z",
     "iopub.status.busy": "2025-12-15T16:44:22.722241Z",
     "iopub.status.idle": "2025-12-15T16:46:59.065796Z",
     "shell.execute_reply": "2025-12-15T16:46:59.064636Z",
     "shell.execute_reply.started": "2025-12-15T16:44:22.722495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def steer_activations(x, r, lam=-1.0):\n",
    "    if torch.all(r == 0).item():\n",
    "        return x\n",
    "\n",
    "    r = r.to(x.device, x.dtype)\n",
    "    r /= r.norm()\n",
    "        \n",
    "    if x.ndim == 4:  # [B, C, H, W]\n",
    "        r = r[None, :, None, None] # shape [B, C, 1, 1]\n",
    "        channel_dim = 1\n",
    "    elif x.ndim == 3: # [B, T, C] (ff layers)\n",
    "        r = r[None, None, :] # shape [B, C]\n",
    "        channel_dim = 2\n",
    "        \n",
    "    dot_product = (x * r).sum(dim=channel_dim, keepdim=True)\n",
    "    \n",
    "    return x + (lam * dot_product * r)\n",
    "\n",
    "\n",
    "def steer_activations_pca(x, r, pca_data: tuple[torch.Tensor, torch.Tensor], lam: float = -1.0):\n",
    "    #print(f'Steering on {x.shape} with {r.shape}')\n",
    "    if torch.all(r == 0).item():\n",
    "        return x\n",
    "\n",
    "    r = r.to(x.device, x.dtype)\n",
    "\n",
    "    pcs = pca_data[0].half().to(x.device)\n",
    "    mean = pca_data[1].half().to(x.device)\n",
    "\n",
    "    #print(f'{x.shape}, {mean.shape}')\n",
    "    #x -= mean\n",
    "\n",
    "    x_compressed = (x - mean) @ pcs\n",
    "    x_compressed = x @ pcs\n",
    "    r_compressed = r @ pcs\n",
    "    r_constructed = (r_compressed @ pcs.T) + mean\n",
    "    \n",
    "    r_compressed /= r_compressed.norm()\n",
    "    r_constructed /= r_constructed.norm()\n",
    "        \n",
    "    if x.ndim == 4:  # [B, C, H, W]\n",
    "        r = r[None, :, None, None] # shape [B, C, 1, 1]\n",
    "        channel_dim = 1\n",
    "    elif x.ndim == 3: # [B, T, C] (attention layers)\n",
    "        r = r[None, None, :] # shape [B, C]\n",
    "        channel_dim = 2\n",
    "        \n",
    "    dot_product = (x_compressed * r_compressed).sum(dim=channel_dim, keepdim=True)\n",
    "    \n",
    "    return x + (lam * dot_product * r_constructed)\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    prompt: str,\n",
    "    guidance: float,\n",
    "    nets: dict,\n",
    "    steering_vectors: dict[str, torch.Tensor],\n",
    "    timesteps: list[int],\n",
    "    inference_steps: int,\n",
    "    lam: float,\n",
    "    pca_dict: dict[str, dict[int, tuple[torch.Tensor, torch.Tensor]]]\n",
    "):\n",
    "    handles = []\n",
    "\n",
    "    current_step = 0\n",
    "\n",
    "    def steering_hook(layer: str, steering_vector: torch.Tensor):\n",
    "        ts_index = 0\n",
    "        \n",
    "        def hook(module, inp, out):\n",
    "            nonlocal ts_index\n",
    "            #print(f\"[STEERING] layer={layer_name} step={current_step}\")\n",
    "\n",
    "            # out can be tensor or (hidden, tensor)\n",
    "            if isinstance(out, tuple):\n",
    "                hidden, activation = out\n",
    "            else:\n",
    "                hidden, activation = None, out  # activation: [B, C, H, W]\n",
    "\n",
    "            if current_step in timesteps: \n",
    "                #print(f'[{layer}] -> Step {current_step}, ts_index {ts_index}')\n",
    "                B = activation.size(0)\n",
    "\n",
    "                assert B % 2 == 0\n",
    "                \n",
    "                x = activation[B // 2 :, :, :]\n",
    "\n",
    "                pca_data = pca_dict.get(layer, {}).get(current_step, None) if pca_dict is not None else None\n",
    "\n",
    "                if pca_data is not None:\n",
    "                    x_steered = steer_activations_pca(x, steering_vector[ts_index], pca_data, lam)\n",
    "                else:\n",
    "                    x_steered = steer_activations(x, steering_vector[ts_index], lam)\n",
    "                \n",
    "                activation[B // 2 :] = x_steered\n",
    "                    \n",
    "                ts_index += 1\n",
    "\n",
    "            if hidden is None:\n",
    "                return activation\n",
    "            else:\n",
    "                return (hidden, activation)\n",
    "\n",
    "        return hook\n",
    "\n",
    "    for layer, steering_vector in steering_vectors.items():\n",
    "        handles.append(\n",
    "            nets[layer].register_forward_hook(steering_hook(layer, steering_vector))\n",
    "        )\n",
    "\n",
    "    def callback(pipeline, step_index, timestep, callback_kwargs):\n",
    "        nonlocal current_step\n",
    "        current_step = step_index\n",
    "\n",
    "        return callback_kwargs\n",
    "    \n",
    "    try:\n",
    "        return pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=inference_steps,\n",
    "            guidance_scale=guidance,\n",
    "            callback_on_step_end=callback,\n",
    "            generator=torch.Generator(device=\"cuda\").manual_seed(362)\n",
    "        ).images\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "\n",
    "# Run generation with steering\n",
    "\n",
    "all_images = []\n",
    "lambdas = []\n",
    "\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "LAMBDA_PARAMS = [-4, -3.5, -3, -2.5, -2, -1.5, -1, 0]\n",
    "K_PARAMS = [1, 3, 5, len(LAYERS)]\n",
    "TIMESTEP_PARAMS = ['f', 'l', 'all']\n",
    "OUTPUT_DIR = './'\n",
    "os.makedirs(f'{OUTPUT_DIR}/base_imgs', exist_ok=True)\n",
    "os.makedirs(f'{OUTPUT_DIR}/steered_imgs', exist_ok=True)\n",
    "\n",
    "\n",
    "for i in range(0, len(dog_prompts), BATCH_SIZE):\n",
    "    prompt_batch = dog_prompts[i:i + BATCH_SIZE]\n",
    "    print(prompt_batch)\n",
    "    for lam in LAMBDA_PARAMS:\n",
    "        if lam == 0:\n",
    "            images = generate_with_steering(\n",
    "                        pipe,\n",
    "                        prompt_batch,\n",
    "                        GUIDANCE,\n",
    "                        nets,\n",
    "                        steering_vectors,\n",
    "                        timesteps=get_timesteps_by_name(STEPS, 'all'),\n",
    "                        inference_steps=STEPS,\n",
    "                        lam=lam,\n",
    "                        pca_dict=None\n",
    "                    )   \n",
    "            for idx, image in enumerate(images):\n",
    "                filename = f'{OUTPUT_DIR}/base_imgs/{i + idx}.png'\n",
    "                image.save(filename)\n",
    "        else:\n",
    "            for k in K_PARAMS:\n",
    "                for t in TIMESTEP_PARAMS:\n",
    "                    top_k_per_timestep = get_top_k_layers(results, k=k)\n",
    "                    filtered_vectors = mask_vectors_by_top_k(\n",
    "                                        steering_vectors, \n",
    "                                        timesteps=get_timesteps_by_name(STEPS, t), \n",
    "                                        top_k_per_timestep=top_k_per_timestep\n",
    "                                    )\n",
    "            \n",
    "                    steered_images = generate_with_steering(\n",
    "                        pipe,\n",
    "                        prompt_batch,\n",
    "                        GUIDANCE,\n",
    "                        nets,\n",
    "                        filtered_vectors, # steering_vectors,\n",
    "                        timesteps=get_timesteps_by_name(STEPS, t),\n",
    "                        inference_steps=STEPS,\n",
    "                        lam=lam,\n",
    "                        pca_dict=None\n",
    "                    )\n",
    "    \n",
    "                    for idx, image in enumerate(steered_images):\n",
    "                        filename = f'{OUTPUT_DIR}/steered_imgs/{i + idx}_lambda={lam}_k={k}_t={t}.png'\n",
    "                        image.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r output_images.zip base_imgs steered_imgs"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8901347,
     "sourceId": 14127634,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from typing import Optional\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", # \"Manojb/stable-diffusion-2-1-base\" # \"CompVis/stable-diffusion-v1-4\",\n",
    "    safety_checker=None,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_unet_nets(pipe):\n",
    "    nets = {}\n",
    "    \n",
    "    for i, block in enumerate(pipe.unet.down_blocks):\n",
    "        #for j, resnet in enumerate(block.resnets):\n",
    "        #    nets[f\"down_block_{i}_resnet_{j}\"] = resnet\n",
    "\n",
    "        if hasattr(block, \"attentions\"):\n",
    "            for j, attn in enumerate(block.attentions):\n",
    "                for k, transformer in enumerate(attn.transformer_blocks):\n",
    "                    name = f\"down_block_{i}_attn_{j}_trans_{k}_attn2\"\n",
    "                    nets[name] = transformer.attn2\n",
    "\n",
    "    \n",
    "    #for j, resnet in enumerate(pipe.unet.mid_block.resnets):\n",
    "    #    nets[f\"mid_block_resnet_{j}\"] = resnet\n",
    "\n",
    "    if hasattr(pipe.unet.mid_block, \"attentions\"):\n",
    "        for j, attn in enumerate(pipe.unet.mid_block.attentions):\n",
    "            for k, transformer in enumerate(attn.transformer_blocks):\n",
    "                name = f\"mid_block_attn_{j}_trans_{k}_attn2\"\n",
    "                nets[name] = transformer.attn2\n",
    "                \n",
    "\n",
    "    for i, block in enumerate(pipe.unet.up_blocks):\n",
    "        #for j, resnet in enumerate(block.resnets):\n",
    "        #    nets[f\"up_block_{i}_resnet_{j}\"] = resnet\n",
    "\n",
    "        if hasattr(block, \"attentions\"):\n",
    "            for j, attn in enumerate(block.attentions):\n",
    "                for k, transformer in enumerate(attn.transformer_blocks):\n",
    "                    name = f\"up_block_{i}_attn_{j}_trans_{k}_attn2\"\n",
    "                    nets[name] = transformer.attn2\n",
    "\n",
    "    return nets\n",
    "\n",
    "\n",
    "nets = get_unet_nets(pipe)\n",
    "print(list(nets.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dogs_dataset = pd.read_csv('/kaggle/input/new-nudity-steering/nudity.csv')\n",
    "\n",
    "dog_prompts = dogs_dataset['positive'].tolist()\n",
    "non_dog_prompts = dogs_dataset['negative'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def collect_residual_streams(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    forget_set: list[str],\n",
    "    retain_set: list[str],\n",
    "    guidance: float,\n",
    "    nets: dict,\n",
    "    layers: list[str],\n",
    "    timesteps: list[int]\n",
    "):\n",
    "    forget_acts = []\n",
    "    retain_acts = []\n",
    "\n",
    "    for idx, (forget_prompt, retain_prompt) in enumerate(zip(forget_set, retain_set)):\n",
    "        print(f'[{idx+1}] Extracting acts for forget prompt: {forget_prompt}')\n",
    "        forget_act = get_unet_residual_stream(pipe, forget_prompt, guidance, nets, layers, timesteps)\n",
    "\n",
    "        print(f'[{idx+1}] Extracting acts for retain prompt: {retain_prompt}')\n",
    "        retain_act = get_unet_residual_stream(pipe, retain_prompt, guidance, nets, layers, timesteps)\n",
    "        \n",
    "        forget_acts.append(forget_act)\n",
    "        retain_acts.append(retain_act)\n",
    "\n",
    "    forget_layers = {}\n",
    "    retain_layers = {}\n",
    "    \n",
    "    for l in layers:\n",
    "        forget_layers[l] = torch.stack([f[l] for f in forget_acts], dim=0)\n",
    "        retain_layers[l] = torch.stack([r[l] for r in retain_acts], dim=0)\n",
    "        \n",
    "    return forget_layers, retain_layers\n",
    "\n",
    "\n",
    "\n",
    "def get_unet_residual_stream(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    prompt: str,\n",
    "    guidance: float,\n",
    "    nets: dict,\n",
    "    layers: list[str],\n",
    "    timesteps: list[int]\n",
    "):\n",
    "    # designed to be simple, using batches would cause coherence issues when collecting acts.\n",
    "    residuals_dict = {}\n",
    "    handles = []\n",
    "\n",
    "    current_step = 0\n",
    "\n",
    "    def save_residuals(name):\n",
    "        def hook(module, input, output):           \n",
    "            if current_step in timesteps:\n",
    "                # UNet calculates noise prediction for both conditioned and unconditioned input, so we take the second\n",
    "                residual = output[1] if isinstance(output, tuple) else output\n",
    "                residuals_dict.setdefault(name, []).append(residual[1].detach().cpu())\n",
    "\n",
    "        return hook\n",
    "\n",
    "    for l in layers:\n",
    "        handles.append(\n",
    "            nets[l].register_forward_hook(save_residuals(l))\n",
    "        )\n",
    "\n",
    "    def callback(pipeline, step_index, timestep, callback_kwargs):\n",
    "        nonlocal current_step\n",
    "        current_step = step_index\n",
    "\n",
    "        return callback_kwargs\n",
    "    \n",
    "    try:\n",
    "        images = pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=timesteps[-1],\n",
    "            guidance_scale=guidance,\n",
    "            callback_on_step_end=callback\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            layer: torch.stack(tensors, dim=0)\n",
    "            for layer, tensors in residuals_dict.items()\n",
    "        } # [T, C, H, W]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "def show_images(images: list[Image.Image], prompts: list[str], cols: int = 2, width: int = 40) -> None:\n",
    "    assert len(images) == len(prompts)\n",
    "\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax in axes[len(images):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax, img, prompt in zip(axes, images, prompts):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        wrapped_prompt = \"\\n\".join(textwrap.wrap(prompt, width=width))\n",
    "        ax.text(0.5, -0.05, wrapped_prompt, fontsize=10, ha='center', va='top', transform=ax.transAxes)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_color('black')\n",
    "            spine.set_linewidth(1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract raw activations and print layer names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(pipe.unet)\n",
    "ALL_LAYERS = list(nets.keys())\n",
    "GUIDANCE = 7.5\n",
    "LAYERS = ALL_LAYERS\n",
    "STEPS = 30\n",
    "TIMESTEPS = list(range(1, STEPS+1))\n",
    "forget_acts, retain_acts = collect_residual_streams(\n",
    "    pipe,\n",
    "    dog_prompts[:10],\n",
    "    non_dog_prompts[:10],\n",
    "    guidance=GUIDANCE,\n",
    "    nets=nets,\n",
    "    layers=LAYERS,\n",
    "    timesteps=TIMESTEPS\n",
    ")\n",
    "\n",
    "#for layer, act in forget_acts.items():\n",
    "#    print(f'Layer {layer}: {act.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_differences(forget_layers_act, retain_layers_act, layer_nav=False):\n",
    "    if layer_nav:\n",
    "        return (forget_layers_act - retain_layers_act).mean(dim=0)\n",
    "        \n",
    "    result = {}\n",
    "    for (layer, X), (_, Y) in zip(forget_layers_act.items(), retain_layers_act.items()):\n",
    "        if X.ndim == 5: # Resnet activation\n",
    "            result[layer] = (X - Y).mean(dim=(0, 3, 4)) # [T, C]\n",
    "        else: # FF net activation\n",
    "            result[layer] = (X - Y).mean(dim=(0, 2))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_acts(acts_dict):\n",
    "    temp_dict = {}\n",
    "    for layer_name, act in acts_dict.items():\n",
    "        # act is a  Tensor(N, steps, C, H, W) for each item in the batch\n",
    "\n",
    "        act = act.to('cuda') # (N, Steps, C, H, W)\n",
    "\n",
    "        if act.ndim == 5: # Resnet activation\n",
    "            spatial_averaged = act.mean(dim=(3,4)) # (N, steps, C)\n",
    "        else: # Feed forward attention activation\n",
    "            spatial_averaged = act.mean(dim=2) # (N, steps, C)\n",
    "            \n",
    "        temp_dict[layer_name] = spatial_averaged.float() \n",
    "\n",
    "    \n",
    "    return temp_dict\n",
    "\n",
    "\n",
    "def compute_scores(retain_acts, forget_acts, timesteps, top_k):\n",
    "    results = {}\n",
    "    retain_acts, forget_acts = process_acts(retain_acts), process_acts(forget_acts)\n",
    "\n",
    "    for timestep in timesteps:\n",
    "        timestep_dict = {}\n",
    "        for layer in retain_acts:\n",
    "            P = retain_acts[layer][:, timestep-timesteps[0], :]  # Positive (N, D)\n",
    "            N = forget_acts[layer][:, timestep-timesteps[0], :]  # Negative (N, D)\n",
    "    \n",
    "            if P.shape != N.shape:\n",
    "                print(f'P shape and N shape differs in {layer}')\n",
    "    \n",
    "            n_samples = P.shape[0]\n",
    "    \n",
    "            all_acts = torch.cat([P, N], dim=0) # (2N, D)\n",
    "            mu_l = all_acts.mean(dim=0, keepdim=True)  # (1, D)\n",
    "            sigma_l = all_acts.std(dim=0, keepdim=True) + 1e-8 # (1, D)\n",
    "    \n",
    "            P_tilde = (P - mu_l) / sigma_l\n",
    "            N_tilde = (N - mu_l) / sigma_l\n",
    "            \n",
    "            v_l = compute_mean_differences(N, P, layer_nav=True) # (D)\n",
    "    \n",
    "            \n",
    "            # Calculate means of normalized data\n",
    "            mu_pos = P_tilde.mean(dim=0) # (D)\n",
    "            mu_neg = N_tilde.mean(dim=0) # (D)\n",
    "    \n",
    "            # Instead of creating (D, D) matrix, project means onto v_l\n",
    "            proj_pos = torch.dot(mu_pos, v_l)\n",
    "            proj_neg = torch.dot(mu_neg, v_l)\n",
    "            \n",
    "            # v^T Sb v = N * (proj_pos^2 + proj_neg^2)\n",
    "            sb_val = n_samples * (proj_pos**2 + proj_neg**2)\n",
    "    \n",
    "            # Center the data class-wise\n",
    "            P_centered = P_tilde - mu_pos.unsqueeze(0) # (N, D)\n",
    "            N_centered = N_tilde - mu_neg.unsqueeze(0) # (N, D)\n",
    "    \n",
    "            # Instead of creating (D, D) covariance, project data onto v_l\n",
    "            # This calculates the variance of the data along the direction of v_l\n",
    "            p_proj = torch.mv(P_centered, v_l) # (N)\n",
    "            n_proj = torch.mv(N_centered, v_l) # (N)\n",
    "    \n",
    "            sw_pos_val = torch.sum(p_proj**2)\n",
    "            sw_neg_val = torch.sum(n_proj**2)\n",
    "            sw_val = sw_pos_val + sw_neg_val\n",
    "    \n",
    "            \n",
    "            D_l = (sb_val / (sb_val + sw_val + 1e-8)).item()\n",
    "    \n",
    "            pair_diffs = N_tilde - P_tilde # (N, D)\n",
    "            dot_products = torch.mv(pair_diffs, v_l) # (N)\n",
    "            pair_norms = torch.norm(pair_diffs, dim=1) # (N,)\n",
    "            v_norm = torch.norm(v_l)\n",
    "            \n",
    "            cosine_sims = dot_products / (pair_norms * v_norm + 1e-8)\n",
    "            C_l = cosine_sims.mean().item()\n",
    "    \n",
    "            S_l = D_l + C_l\n",
    "    \n",
    "            timestep_dict[layer] = {\n",
    "                \"score\": S_l,\n",
    "                \"discriminability\": D_l,\n",
    "                \"consistency\": C_l\n",
    "            }\n",
    "            \n",
    "            del P_tilde, N_tilde, all_acts, P_centered, N_centered\n",
    "            torch.cuda.empty_cache()\n",
    "        sorted_layers = sorted(timestep_dict.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "        results[timestep] = [x for x in sorted_layers[:top_k]]\n",
    "    return results\n",
    "\n",
    "def get_top_k_layers(results):\n",
    "    res = {}\n",
    "    for timestep, top in results.items():\n",
    "        res[timestep] = [x[0] for x in top]\n",
    "    return res\n",
    "\n",
    "def print_report(results):\n",
    "    for timestep, top in results.items():\n",
    "        print(f\"Timestep: {timestep}\")\n",
    "        for layer, score_dict in top:\n",
    "            score, disc, cons = score_dict['score'], score_dict['discriminability'], score_dict['consistency']\n",
    "            print(f'\\tLayer: {layer} | Score: {score} | Disc: {disc} | Cons: {cons}')\n",
    "\n",
    "def mask_vectors_by_top_k(steering_vectors, timesteps, top_k_per_timestep):\n",
    "    masked_vectors = {l: v.clone() for l, v in steering_vectors.items()}\n",
    "    \n",
    "    for ts_index, step in enumerate(timesteps):\n",
    "        active_layers = top_k_per_timestep.get(step, [])\n",
    "        \n",
    "        for layer_name, vector_tensor in masked_vectors.items():\n",
    "            # If this layer is NOT in the active list for this step, zero it out\n",
    "            if layer_name not in active_layers:\n",
    "                vector_tensor[ts_index] = 0.0\n",
    "\n",
    "    return masked_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = compute_scores(retain_acts, forget_acts, timesteps=TIMESTEPS[:-1], top_k=5)\n",
    "\n",
    "top_k_per_timestep = get_top_k_layers(results)\n",
    "print_report(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate steering vectors\n",
    "steering_vectors = compute_mean_differences(forget_acts, retain_acts)\n",
    "\n",
    "filtered_vectors = mask_vectors_by_top_k(\n",
    "    steering_vectors, \n",
    "    timesteps=TIMESTEPS[:-1], \n",
    "    top_k_per_timestep=top_k_per_timestep\n",
    ")\n",
    "print([(layer, steering_vector.shape) for (layer, steering_vector) in steering_vectors.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def steer_activations(x, r, lam=-1.0):\n",
    "    #print(f'Steering on {x.shape} with {r.shape}')\n",
    "    if torch.all(r == 0).item():\n",
    "        return x\n",
    "        \n",
    "    r = r.to(x.device, x.dtype)\n",
    "    r = r/r.norm()\n",
    "    \n",
    "    if x.ndim == 3:  # [C, H, W]\n",
    "        r = r[:, None, None]      # shape [C, 1, 1]\n",
    "        channel_dim = 0\n",
    "    elif x.ndim == 4:  # [1, C, H, W]\n",
    "        r = r[None, :, None, None] # shape [1, C, 1, 1]\n",
    "        channel_dim = 1\n",
    "    else: # [S, C] (ff layers)\n",
    "        r = r[None, :] # shape [1, C]\n",
    "        channel_dim = 1\n",
    "        \n",
    "    \n",
    "    dot_product = (x * r).sum(dim=channel_dim, keepdim=True)\n",
    "    \n",
    "    return x + lam * (dot_product * r)\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    prompt: str,\n",
    "    guidance: float,\n",
    "    nets: dict,\n",
    "    steering_vectors: dict[str, torch.Tensor],\n",
    "    timesteps: list[int],\n",
    "    lam: float,\n",
    "):\n",
    "    # designed to be simple, using batches would cause coherence issues when collecting acts.\n",
    "    residuals_dict = {}\n",
    "    handles = []\n",
    "\n",
    "    current_step = 0\n",
    "\n",
    "    def steering_hook(layer: str, steering_vector: torch.Tensor):\n",
    "        ts_index = 0\n",
    "        \n",
    "        def hook(module, inp, out):\n",
    "            nonlocal ts_index\n",
    "            #print(f\"[STEERING] layer={layer_name} step={current_step}\")\n",
    "\n",
    "            # out can be tensor or (hidden, tensor)\n",
    "            if isinstance(out, tuple):\n",
    "                hidden, residual = out\n",
    "            else:\n",
    "                hidden, residual = None, out  # residual: [B, C, H, W]\n",
    "                \n",
    "            if current_step in timesteps: \n",
    "                #print(f'[{layer}] -> Step {current_step}, ts_index {ts_index}')\n",
    "                x = residual[1]\n",
    "                x_steered = steer_activations(x, steering_vector[ts_index], lam)\n",
    "                residual[1] = x_steered\n",
    "                    \n",
    "                ts_index += 1\n",
    "\n",
    "            if hidden is None:\n",
    "                return residual\n",
    "            else:\n",
    "                return (hidden, residual)\n",
    "\n",
    "        return hook\n",
    "\n",
    "    for layer, steering_vector in steering_vectors.items():\n",
    "        handles.append(\n",
    "            nets[layer].register_forward_hook(steering_hook(layer, steering_vector))\n",
    "        )\n",
    "\n",
    "    def callback(pipeline, step_index, timestep, callback_kwargs):\n",
    "        nonlocal current_step\n",
    "        current_step = step_index\n",
    "\n",
    "        return callback_kwargs\n",
    "    \n",
    "    try:\n",
    "        return pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=timesteps[-1],\n",
    "            guidance_scale=guidance,\n",
    "            callback_on_step_end=callback,\n",
    "            generator=torch.Generator(device=\"cuda\").manual_seed(362)\n",
    "        ).images\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "\n",
    "# Run generation with steering\n",
    "prompt = dog_prompts[1]\n",
    "\n",
    "all_images = []\n",
    "lambdas = []\n",
    "print(prompt)\n",
    "for lam in torch.arange(-3, 3, 0.5):\n",
    "    steered_images = generate_with_steering(\n",
    "        pipe,\n",
    "        prompt,\n",
    "        GUIDANCE,\n",
    "        nets,\n",
    "        filtered_vectors, # steering_vectors,\n",
    "        timesteps=TIMESTEPS,\n",
    "        lam=lam,\n",
    "    )\n",
    "    all_images.extend(steered_images)\n",
    "    lambdas.append(str(lam.item()))\n",
    "\n",
    "# Visualize\n",
    "show_images(all_images, lambdas, cols=3)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8976063,
     "sourceId": 14095498,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

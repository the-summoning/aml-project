{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13963606,"sourceType":"datasetVersion","datasetId":8901347}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport math\n\nimport textwrap\n\nfrom typing import Optional\nfrom diffusers import StableDiffusionPipeline\nfrom huggingface_hub import notebook_login\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nnotebook_login()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = StableDiffusionPipeline.from_pretrained(\n    \"Manojb/stable-diffusion-2-1-base\", # \"CompVis/stable-diffusion-v1-4\",\n    safety_checker=None,\n    torch_dtype=torch.float16\n).to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:19:34.808965Z","iopub.execute_input":"2025-12-04T16:19:34.809591Z","iopub.status.idle":"2025-12-04T16:19:37.875836Z","shell.execute_reply.started":"2025-12-04T16:19:34.809565Z","shell.execute_reply":"2025-12-04T16:19:37.875213Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46c8709ccb745508f22b139c9f43d41"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def collect_residual_streams(\n    pipe: StableDiffusionPipeline,\n    forget_set: list[str],\n    retain_set: list[str],\n    steps: int = 30,\n    guidance: float = 7.5,\n    from_timestamp: int = 25\n):\n    \n    forget_acts = []\n    retain_acts = []\n\n    for idx, (forget_prompt, retain_prompt) in enumerate(zip(forget_set, retain_set)):\n        print(f'[{idx+1}] Extracting acts for forget prompt: {forget_prompt}')\n        forget_act = get_unet_residual_stream(pipe, forget_prompt, steps, guidance, from_timestamp)\n\n        print(f'[{idx+1}] Extracting acts for retain prompt: {retain_prompt}')\n        retain_act = get_unet_residual_stream(pipe, retain_prompt, steps, guidance, from_timestamp)\n        \n        forget_acts.append(forget_act)\n        retain_acts.append(retain_act)\n\n    return forget_acts, retain_acts\n\n\ndef get_unet_residual_stream(\n    pipe: StableDiffusionPipeline,\n    prompt: str,\n    steps: int = 30,\n    guidance: float = 7.5,\n    from_timestamp: int = 25\n):\n    # designed to be, using batches would cause coherence issues when collecting acts.    \n\n    assert 0 < from_timestamp < steps\n    \n    residuals_dict = {}\n    handles = []\n\n    def save_residuals(name):\n        def hook(module, input, output):\n            # UNet calculates noise prediction for both conditioned and unconditioned input, so we take the second\n            residual = output[1] if isinstance(output, tuple) else output\n            residuals_dict.setdefault(name, []).append(residual[1].detach().cpu())\n        \n        return hook\n\n    for i, block in enumerate(pipe.unet.down_blocks):\n        for j, resnet in enumerate(block.resnets):\n            handles.append(resnet.register_forward_hook(save_residuals(f\"down_block_{i}_resnet_{j}\")))\n\n    for j, resnet in enumerate(pipe.unet.mid_block.resnets):\n        handles.append(resnet.register_forward_hook(save_residuals(f\"mid_block_resnet_{j}\")))\n\n    for i, block in enumerate(pipe.unet.up_blocks):\n        for j, resnet in enumerate(block.resnets):\n            handles.append(resnet.register_forward_hook(save_residuals(f\"up_block_{i}_resnet_{j}\")))\n\n    pipe(\n        prompt,\n        num_inference_steps=steps,\n        guidance_scale=guidance\n    )\n\n    for h in handles:\n        h.remove()\n\n    residuals_by_timestep = {\n        layer: torch.stack(tensors, dim=0)[from_timestamp:]\n        for layer, tensors in residuals_dict.items()\n    }\n\n    return residuals_by_timestep # [T, C, H, W]\n\ndef show_images(images: list[Image.Image], prompts: list[str], cols: int = 2, width: int = 40) -> None:\n    assert len(images) == len(prompts)\n\n    rows = math.ceil(len(images) / cols)\n    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n\n    if isinstance(axes, np.ndarray):\n        axes = axes.flatten()\n    else:\n        axes = [axes]\n\n    for ax in axes[len(images):]:\n        ax.axis('off')\n\n    for ax, img, prompt in zip(axes, images, prompts):\n        ax.imshow(img)\n        ax.axis('off')\n        wrapped_prompt = \"\\n\".join(textwrap.wrap(prompt, width=width))\n        ax.text(0.5, -0.05, wrapped_prompt, fontsize=10, ha='center', va='top', transform=ax.transAxes)\n        for spine in ax.spines.values():\n            spine.set_visible(True)\n            spine.set_color('black')\n            spine.set_linewidth(1)\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef get_layers_activations_at_timestep(forget_acts, retain_acts, layers: list[str], ts_idx: int):\n    result = {}\n\n    forget_layers = {}\n    retain_layers = {}\n    \n    for l in layers:\n        forget_layers[l] = torch.stack([f[l][ts_idx] for f in forget_acts], dim=0)\n        retain_layers[l] = torch.stack([r[l][ts_idx] for r in retain_acts], dim=0)\n        \n    return forget_layers, retain_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:20:15.416651Z","iopub.execute_input":"2025-12-04T16:20:15.416991Z","iopub.status.idle":"2025-12-04T16:20:15.431596Z","shell.execute_reply.started":"2025-12-04T16:20:15.416968Z","shell.execute_reply":"2025-12-04T16:20:15.430502Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"dogs_dataset = pd.read_csv('/kaggle/input/prompts-steering/dogs.csv')\n\ndog_prompts = dogs_dataset['positive'].tolist()\nnon_dog_prompts = dogs_dataset['negative'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Generate a bunch of images just to visualize them**","metadata":{}},{"cell_type":"code","source":"batch_size = 2\nimages_no = 6 # len(dog_prompts)\n\nall_images = []\nall_prompts = []\n\nfor i in range(0, images_no, batch_size):\n    pos_batch = dog_prompts[i:i+batch_size]\n    neg_batch = non_dog_prompts[i:i+batch_size]\n\n    pos_images = pipe(pos_batch, num_inference_steps=30, guidance_scale=8).images\n    neg_images = pipe(neg_batch, num_inference_steps=30, guidance_scale=8).images\n\n    for p_img, n_img, p_prompt, n_prompt in zip(pos_images, neg_images, pos_batch, neg_batch):\n        all_images.append(p_img)\n        all_images.append(n_img)\n        all_prompts.append(p_prompt)\n        all_prompts.append(n_prompt)\n\nshow_images(all_images, all_prompts, cols=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extract raw activations and print layer names**","metadata":{}},{"cell_type":"code","source":"#print(pipe.unet)\nforget_acts, retain_acts = collect_residual_streams(pipe, dog_prompts[:5], non_dog_prompts[:5], 30, 10, 29)\n\nlayer_names = set(forget_acts[0].keys()) # take first prompt, same layer names...\n\nlayer_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T15:55:09.929655Z","iopub.execute_input":"2025-12-04T15:55:09.930223Z","iopub.status.idle":"2025-12-04T15:55:56.601998Z","shell.execute_reply.started":"2025-12-04T15:55:09.930198Z","shell.execute_reply":"2025-12-04T15:55:56.601250Z"}},"outputs":[{"name":"stdout","text":"[1] Extracting acts for forget prompt: A playful golden retriever running in a sunny park, photorealistic\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec9521356344b819f1a7e593d8acbb9"}},"metadata":{}},{"name":"stdout","text":"[1] Extracting acts for retain prompt: A playful child running in a sunny park, photorealistic\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83e181fec6f4e7fb321aa90a89c3cd5"}},"metadata":{}},{"name":"stdout","text":"[2] Extracting acts for forget prompt: A group of dogs playing in the snow, winter scene, cinematic lighting\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b23d27d1801490ca5fa8fdb273dfe22"}},"metadata":{}},{"name":"stdout","text":"[2] Extracting acts for retain prompt: A group of children playing in the snow, winter scene, cinematic lighting\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4fa8bef5c484b239734fd242f82db05"}},"metadata":{}},{"name":"stdout","text":"[3] Extracting acts for forget prompt: Close-up of a dog’s face, detailed fur and expressive eyes, portrait\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf82afc52c2448b96c737cd718544a0"}},"metadata":{}},{"name":"stdout","text":"[3] Extracting acts for retain prompt: Close-up of a cat’s face, detailed fur and expressive eyes, portrait\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd44a77d335e459cbffd5a956e6f958a"}},"metadata":{}},{"name":"stdout","text":"[4] Extracting acts for forget prompt: A dog catching a frisbee in mid-air, dynamic sports shot\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3f2802bdae94a2a8eb5706194d1dec5"}},"metadata":{}},{"name":"stdout","text":"[4] Extracting acts for retain prompt: A boy catching a frisbee in mid-air, dynamic sports shot\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8607090a56114c17a19a6dcba32d7f3a"}},"metadata":{}},{"name":"stdout","text":"[5] Extracting acts for forget prompt: Watercolor painting of a happy dog in a garden, soft pastel colors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"813dcbe862224df6831bb3dfbc26ff41"}},"metadata":{}},{"name":"stdout","text":"[5] Extracting acts for retain prompt: Watercolor painting of a flower in a garden, soft pastel colors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"606e200ac29a46d198e89ef97a6fb658"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'down_block_0_resnet_0',\n 'down_block_0_resnet_1',\n 'down_block_1_resnet_0',\n 'down_block_1_resnet_1',\n 'down_block_2_resnet_0',\n 'down_block_2_resnet_1',\n 'down_block_3_resnet_0',\n 'down_block_3_resnet_1',\n 'mid_block_resnet_0',\n 'mid_block_resnet_1',\n 'up_block_0_resnet_0',\n 'up_block_0_resnet_1',\n 'up_block_0_resnet_2',\n 'up_block_1_resnet_0',\n 'up_block_1_resnet_1',\n 'up_block_1_resnet_2',\n 'up_block_2_resnet_0',\n 'up_block_2_resnet_1',\n 'up_block_2_resnet_2',\n 'up_block_3_resnet_0',\n 'up_block_3_resnet_1',\n 'up_block_3_resnet_2'}"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"**Daniele insert your code here -> Layer Navigator**\nLayer navigator should return a subset of the layers just printed, possibly a dict with scores for report/debug purposes\n\nexample: layer_navigator(...) -> ['down_block_2_resnet_1', 'mid_block_resnet_0', 'mid_block_resnet_1', 'up_block_2_resnet_2']","metadata":{}},{"cell_type":"code","source":"# daniele code","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get timestep specific activations for each specified layer\n\nforget_layers_act, retain_layers_act = get_layers_activations_at_timestep(\n    forget_acts, \n    retain_acts, \n    ['down_block_2_resnet_1', 'mid_block_resnet_0', 'mid_block_resnet_1', 'up_block_2_resnet_2'],\n    -1 # we take the last timestep\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:20:31.125178Z","iopub.execute_input":"2025-12-04T16:20:31.125742Z","iopub.status.idle":"2025-12-04T16:20:31.135019Z","shell.execute_reply.started":"2025-12-04T16:20:31.125715Z","shell.execute_reply":"2025-12-04T16:20:31.134360Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def mean_difference(X, Y, normalize=True):\n    X_mean = X.mean(dim=0) # [C, H, W]\n    Y_mean = Y.mean(dim=0) # [C, H, W]\n\n    v = X_mean - Y_mean\n\n    if normalize:\n        v = v / v.norm()\n\n    return v\n\n\ndef compute_mean_differences(forget_layers_act, retain_layers_act, normalize=True):\n    result = {}\n    for (layer, f), (_, r) in zip(forget_layers_act.items(), retain_layers_act.items()):\n        result[layer] = mean_difference(f, r, normalize)\n        \n    return result\n\n\ndef contrastive_pca(X, Y, n_components=300, alpha=0.8):\n    B, C, H, W = X.shape\n    D = C * H * W\n    \n    X_flat = X.float().reshape(X.shape[0], -1).to('cuda') # (Bx, D)\n    Y_flat = Y.float().reshape(Y.shape[0], -1).to('cuda') # (By, D)\n    \n    mean_X = X_flat.mean(dim=0, keepdim=True)\n    mean_Y = Y_flat.mean(dim=0, keepdim=True)\n    \n    X_centered = X_flat - mean_X\n    Y_centered = Y_flat - mean_Y\n\n    U, S, Vh = torch.linalg.svd(X_centered, full_matrices=False)\n    V = Vh.T\n    \n    X_proj = U @ torch.diag(S)\n    Y_proj = Y_centered @ V\n    \n    Cx_small = (X_proj.T @ X_proj) / (X.shape[0] - 1)\n    Cy_small = (Y_proj.T @ Y_proj) / (Y.shape[0] - 1)\n    \n    C_dual = Cx_small - alpha * Cy_small\n    \n    eigvals, eigvecs_small = torch.linalg.eigh(C_dual)\n    \n    idx = torch.argsort(eigvals, descending=True)[:n_components]\n    top_vecs_small = eigvecs_small[:, idx]\n    \n    components = V @ top_vecs_small\n    \n    components = components / components.norm(dim=0, keepdim=True)\n    \n    return components.T\n\ndef compute_principal_componets(forget_layers_act, retain_layers_act, n_components=10, alpha=1e-3, whiten=False):\n    result = {}\n    for (layer, f), (_, r) in zip(forget_layers_act.items(), retain_layers_act.items()):\n        result[layer] = contrastive_pca(f, r, n_components, alpha, whiten)\n        \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:20:33.016790Z","iopub.execute_input":"2025-12-04T16:20:33.017362Z","iopub.status.idle":"2025-12-04T16:20:33.026268Z","shell.execute_reply.started":"2025-12-04T16:20:33.017336Z","shell.execute_reply":"2025-12-04T16:20:33.025512Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# THE PROBLEM IS THAT WITH PCA we have N <<< D\n# Specifying 300 components to extract but passing a batch of 5-10-30 samples does not make many sense\n\n# components = contrastive_pca(\n#     forget_layers_act['down_block_2_resnet_1'],\n#     retain_layers_act['down_block_2_resnet_1'],\n#     300,\n#     0.8\n# )\n\n# components.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:14:08.879366Z","iopub.execute_input":"2025-12-04T16:14:08.880066Z","iopub.status.idle":"2025-12-04T16:14:08.883390Z","shell.execute_reply.started":"2025-12-04T16:14:08.880037Z","shell.execute_reply":"2025-12-04T16:14:08.882726Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"mean_differences = compute_mean_differences(forget_layers_act, retain_layers_act, True)\n\nprint([(layer, steering_vector.shape) for (layer, steering_vector) in mean_differences.items()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:22:15.036930Z","iopub.execute_input":"2025-12-04T16:22:15.037623Z","iopub.status.idle":"2025-12-04T16:22:15.057448Z","shell.execute_reply.started":"2025-12-04T16:22:15.037596Z","shell.execute_reply":"2025-12-04T16:22:15.056805Z"}},"outputs":[{"name":"stdout","text":"[('down_block_2_resnet_1', torch.Size([1280, 16, 16])), ('mid_block_resnet_0', torch.Size([1280, 8, 8])), ('mid_block_resnet_1', torch.Size([1280, 8, 8])), ('up_block_2_resnet_2', torch.Size([640, 32, 32]))]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"**OLJA, inject you inference code here, mean differences contain the steering vector for each layer specified above**\nFirst try simple mean difference, then we can try to use PCA","metadata":{}},{"cell_type":"code","source":"# olja, code","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
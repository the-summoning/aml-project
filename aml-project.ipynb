{"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14127634,"sourceType":"datasetVersion","datasetId":8901347}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport math\nimport json\n\nimport textwrap\n\nfrom typing import Optional\nfrom diffusers import StableDiffusionPipeline\nfrom huggingface_hub import notebook_login\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom collections import defaultdict\nimport os\nnotebook_login()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-12-15T16:02:17.175578Z","iopub.status.busy":"2025-12-15T16:02:17.174872Z","iopub.status.idle":"2025-12-15T16:02:44.419492Z","shell.execute_reply":"2025-12-15T16:02:44.418708Z","shell.execute_reply.started":"2025-12-15T16:02:17.175547Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", \n    safety_checker=None,\n    torch_dtype=torch.float16\n).to(\"cuda\")","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:03:10.702674Z","iopub.status.busy":"2025-12-15T16:03:10.702165Z","iopub.status.idle":"2025-12-15T16:03:24.498400Z","shell.execute_reply":"2025-12-15T16:03:24.497575Z","shell.execute_reply.started":"2025-12-15T16:03:10.702651Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_unet_layers(pipe, extract_resnet=False, extract_attentions=True):\n    assert extract_resnet or extract_attentions\n    \n    nets = {}\n    \n    for i, block in enumerate(pipe.unet.down_blocks):\n        if extract_resnet:\n            for j, resnet in enumerate(block.resnets):\n               nets[f\"down_block_{i}_resnet_{j}\"] = resnet\n\n        if hasattr(block, \"attentions\") and extract_attentions:\n            for j, attn in enumerate(block.attentions):\n                for k, transformer in enumerate(attn.transformer_blocks):\n                    name = f\"down_block_{i}_attn_{j}_trans_{k}_attn2\" # Cross-attention\n                    nets[name] = transformer.attn2\n                    name = f\"down_block_{i}_attn_{j}_trans_{k}_attn1\" # Self-attention\n                    nets[name] = transformer.attn1\n                    name = f\"down_block_{i}_attn_{j}_trans_{k}_ff\"\n                    nets[name] = transformer.ff\n\n    if extract_resnet:\n        for j, resnet in enumerate(pipe.unet.mid_block.resnets):\n            nets[f\"mid_block_resnet_{j}\"] = resnet\n\n    if hasattr(pipe.unet.mid_block, \"attentions\") and extract_attentions:\n        for j, attn in enumerate(pipe.unet.mid_block.attentions):\n            for k, transformer in enumerate(attn.transformer_blocks):\n                name = f\"mid_block_attn_{j}_trans_{k}_attn2\" # Cross-attention\n                nets[name] = transformer.attn2\n                name = f\"mid_block_attn_{j}_trans_{k}_attn1\" # Self-attention\n                nets[name] = transformer.attn1\n                name = f\"mid_block_attn_{j}_trans_{k}_ff\"\n                nets[name] = transformer.ff\n                \n    \n    for i, block in enumerate(pipe.unet.up_blocks):\n        if extract_resnet:\n            for j, resnet in enumerate(block.resnets):\n                nets[f\"up_block_{i}_resnet_{j}\"] = resnet\n\n        if hasattr(block, \"attentions\") and extract_attentions:\n            for j, attn in enumerate(block.attentions):\n                for k, transformer in enumerate(attn.transformer_blocks):\n                    name = f\"up_block_{i}_attn_{j}_trans_{k}_attn2\" # Cross-attention\n                    nets[name] = transformer.attn2\n                    name = f\"up_block_{i}_attn_{j}_trans_{k}_attn1\" # Self-attention\n                    nets[name] = transformer.attn1\n                    name = f\"up_block_{i}_attn_{j}_trans_{k}_ff\"\n                    nets[name] = transformer.ff\n\n    return nets\n\n\nnets = get_unet_layers(pipe, True, True)\n\nALL_LAYERS = list(nets.keys())\nRESNET_LAYERS = [l for l in ALL_LAYERS if 'resnet_' in l]\nATTENTION_LAYERS = [l for l in ALL_LAYERS if 'attn_' in l]\n\nprint('Resnet layers:', RESNET_LAYERS, end='\\n\\n')\nprint('Attention layers:', ATTENTION_LAYERS)","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:03:29.987619Z","iopub.status.busy":"2025-12-15T16:03:29.987341Z","iopub.status.idle":"2025-12-15T16:03:29.997073Z","shell.execute_reply":"2025-12-15T16:03:29.996483Z","shell.execute_reply.started":"2025-12-15T16:03:29.987597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extraction_dataset = pd.read_csv()\n\nforget_prompts = extraction_dataset['positive'].tolist()[:50]\nretain_prompts = extraction_dataset['negative'].tolist()[:50]\n\n\neval_dataset = pd.read_csv()['prompt'].tolist()[:100]\n","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:03:32.842717Z","iopub.status.busy":"2025-12-15T16:03:32.842078Z","iopub.status.idle":"2025-12-15T16:03:37.405408Z","shell.execute_reply":"2025-12-15T16:03:37.404625Z","shell.execute_reply.started":"2025-12-15T16:03:32.842693Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collect_average_activations(\n    pipe: StableDiffusionPipeline,\n    forget_set: list[str],\n    retain_set: list[str],\n    total_steps: int,\n    guidance: float,\n    nets: dict,\n    layers: list[str],\n    timesteps: list[int]\n):\n    forget_acts = []\n    retain_acts = []\n\n    for idx, (forget_prompt, retain_prompt) in enumerate(zip(forget_set, retain_set)):\n        print(f'[{idx+1}] Extracting acts for forget prompt: {forget_prompt}')\n        forget_act = get_average_activations(pipe, forget_prompt, total_steps, guidance, nets, layers, timesteps)\n\n        print(f'[{idx+1}] Extracting acts for retain prompt: {retain_prompt}')\n        retain_act = get_average_activations(pipe, retain_prompt, total_steps, guidance, nets, layers, timesteps)\n        \n        forget_acts.append(forget_act)\n        retain_acts.append(retain_act)\n\n    forget_layers = {}\n    retain_layers = {}\n    \n    for l in layers:\n        forget_layers[l] = torch.stack([f[l] for f in forget_acts], dim=0)\n        retain_layers[l] = torch.stack([r[l] for r in retain_acts], dim=0)\n        \n    return forget_layers, retain_layers\n\n\n\ndef get_average_activations(\n    pipe: StableDiffusionPipeline,\n    prompt: str,\n    total_steps: int,\n    guidance: float,\n    nets: dict,\n    layers: list[str],\n    timesteps: list[int]\n):\n    # designed to be simple, using batches would cause coherence issues when collecting acts.\n    result = {}\n    handles = []\n\n    current_step = 0\n\n    def save_act(name):\n        def hook(module, input, output):           \n            if current_step in timesteps:\n                # UNet calculates noise prediction for both conditioned and unconditioned input, so we take the second\n                residual = output[1] if isinstance(output, tuple) else output\n\n                if residual[1].ndim == 3: # Channels x Width x Height\n                    act = residual[1].mean(dim=(1, 2)).detach().cpu()\n                elif residual[1].ndim == 2: # Tokens x Context \n                    act = residual[1].mean(dim=0).detach().cpu()\n                else:\n                    raise Exception(f'Unexpected activation shape {residual[1].shape} for {name}') \n                \n                result.setdefault(name, []).append(act)\n                \n        return hook\n\n    for l in layers:\n        handles.append(\n            nets[l].register_forward_hook(save_act(l))\n        )\n\n    def callback(pipeline, step_index, timestep, callback_kwargs):\n        nonlocal current_step\n        current_step = step_index\n\n        return callback_kwargs\n    \n    try:\n        images = pipe(\n            prompt,\n            num_inference_steps=total_steps,\n            guidance_scale=guidance,\n            callback_on_step_end=callback\n        )\n        \n        return {\n            layer: torch.stack(tensors, dim=0)\n            for layer, tensors in result.items()\n        } # [T, C, H, W]\n    except Exception as e:\n        raise e\n    finally:\n        for h in handles:\n            h.remove()\n\ndef show_images(images: list[Image.Image], prompts: list[str], cols: int = 2, width: int = 40) -> None:\n    assert len(images) == len(prompts)\n\n    rows = math.ceil(len(images) / cols)\n    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n\n    if isinstance(axes, np.ndarray):\n        axes = axes.flatten()\n    else:\n        axes = [axes]\n\n    for ax in axes[len(images):]:\n        ax.axis('off')\n\n    for ax, img, prompt in zip(axes, images, prompts):\n        ax.imshow(img)\n        ax.axis('off')\n        wrapped_prompt = \"\\n\".join(textwrap.wrap(prompt, width=width))\n        ax.text(0.5, -0.05, wrapped_prompt, fontsize=10, ha='center', va='top', transform=ax.transAxes)\n        for spine in ax.spines.values():\n            spine.set_visible(True)\n            spine.set_color('black')\n            spine.set_linewidth(1)\n\n    plt.tight_layout()\n    plt.show()\n\ndef get_timesteps_by_name(steps, name: str):\n    if name == 'f':\n        return list(range(0, steps // 2 + 1))\n    elif name == 'l':\n        return list(range(steps // 2, steps + 1))\n    elif name == 'all':\n        return list(range(0, steps + 1))\n    else:\n        raise ValueError(f'{name}')","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:52:13.404716Z","iopub.status.busy":"2025-12-15T16:52:13.404174Z","iopub.status.idle":"2025-12-15T16:52:13.418479Z","shell.execute_reply":"2025-12-15T16:52:13.417568Z","shell.execute_reply.started":"2025-12-15T16:52:13.404691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GUIDANCE = 7.5\nLAYERS = ALL_LAYERS\nSTEPS = 30\nTIMESTEPS_NAME = 'all'\nTIMESTEPS = get_timesteps_by_name(STEPS, TIMESTEPS_NAME)\nLAYER_NAV_K = len(LAYERS)","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:52:15.284083Z","iopub.status.busy":"2025-12-15T16:52:15.283574Z","iopub.status.idle":"2025-12-15T16:52:15.287743Z","shell.execute_reply":"2025-12-15T16:52:15.287134Z","shell.execute_reply.started":"2025-12-15T16:52:15.284062Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forget_acts, retain_acts = collect_average_activations(\n    pipe,\n    forget_prompts,\n    retain_prompts,\n    total_steps=STEPS,\n    guidance=GUIDANCE,\n    nets=nets,\n    layers=LAYERS,\n    timesteps=TIMESTEPS\n)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:03:43.059812Z","iopub.status.busy":"2025-12-15T16:03:43.059325Z","iopub.status.idle":"2025-12-15T16:08:59.120756Z","shell.execute_reply":"2025-12-15T16:08:59.119972Z","shell.execute_reply.started":"2025-12-15T16:03:43.059786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_scores(retain_acts, forget_acts, timesteps, top_k):\n    results = {}\n\n    for timestep in timesteps :\n        timestep_dict = {}\n        for layer in retain_acts:\n            P = retain_acts[layer][:, timestep-timesteps[0], :].float()  # Positive (N, D)\n            N = forget_acts[layer][:, timestep-timesteps[0], :].float()  # Negative (N, D)\n    \n            if P.shape != N.shape:\n                print(f'P shape and N shape differs in {layer}')\n    \n            n_samples = P.shape[0]\n    \n            all_acts = torch.cat([P, N], dim=0) # (2N, D)\n            mu_l = all_acts.mean(dim=0, keepdim=True)  # (1, D)\n            sigma_l = all_acts.std(dim=0, keepdim=True) + 1e-8 # (1, D)\n    \n            P_tilde = (P - mu_l) / sigma_l\n            N_tilde = (N - mu_l) / sigma_l\n\n            v_l = (N - P).mean(dim=0)\n    \n            \n            # Calculate means of normalized data\n            mu_pos = P_tilde.mean(dim=0) # (D)\n            mu_neg = N_tilde.mean(dim=0) # (D)\n    \n            # Instead of creating (D, D) matrix, project means onto v_l\n            proj_pos = torch.dot(mu_pos, v_l)\n            proj_neg = torch.dot(mu_neg, v_l)\n            \n            # v^T Sb v = N * (proj_pos^2 + proj_neg^2)\n            sb_val = n_samples * (proj_pos**2 + proj_neg**2)\n    \n            # Center the data class-wise\n            P_centered = P_tilde - mu_pos.unsqueeze(0) # (N, D)\n            N_centered = N_tilde - mu_neg.unsqueeze(0) # (N, D)\n    \n            # Instead of creating (D, D) covariance, project data onto v_l\n            # This calculates the variance of the data along the direction of v_l\n            p_proj = torch.mv(P_centered, v_l) # (N)\n            n_proj = torch.mv(N_centered, v_l) # (N)\n    \n            sw_pos_val = torch.sum(p_proj**2)\n            sw_neg_val = torch.sum(n_proj**2)\n            sw_val = sw_pos_val + sw_neg_val\n    \n            \n            D_l = (sb_val / (sb_val + sw_val + 1e-8)).item()\n    \n            pair_diffs = N_tilde - P_tilde # (N, D)\n            dot_products = torch.mv(pair_diffs, v_l) # (N)\n            pair_norms = torch.norm(pair_diffs, dim=1) \n            v_norm = torch.norm(v_l)\n            \n            cosine_sims = dot_products / (pair_norms * v_norm + 1e-8)\n            C_l = cosine_sims.mean().item()\n    \n            S_l = D_l + C_l\n    \n            timestep_dict[layer] = {\n                \"score\": S_l,\n                \"discriminability\": D_l,\n                \"consistency\": C_l\n            }\n            \n            del P_tilde, N_tilde, all_acts, P_centered, N_centered\n            torch.cuda.empty_cache()\n            \n        sorted_layers = sorted(timestep_dict.items(), key=lambda x: x[1]['score'], reverse=True)\n        results[timestep] = [x for x in sorted_layers[:top_k]]\n        \n    return results\n\ndef get_top_k_layers(results, k):\n    res = {}\n    for timestep, top in results.items():\n        res[timestep] = [x[0] for x in top[:k]]\n    return res\n\ndef save_report(results, path):\n    with open(path, 'w') as f:\n        json.dump(results, f, indent=4)\n\ndef mask_vectors_by_top_k(steering_vectors, timesteps, top_k_per_timestep):\n    masked_vectors = {l: v.clone() for l, v in steering_vectors.items()}\n    \n    for ts_index, step in enumerate(timesteps):\n        active_layers = top_k_per_timestep.get(step, [])\n        \n        for layer_name, vector_tensor in masked_vectors.items():\n            # If this layer is NOT in the active list for this step, zero it out\n            if layer_name not in active_layers:\n                vector_tensor[ts_index] = 0.0\n\n    return masked_vectors","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:09:33.789343Z","iopub.status.busy":"2025-12-15T16:09:33.789046Z","iopub.status.idle":"2025-12-15T16:09:33.801659Z","shell.execute_reply":"2025-12-15T16:09:33.801039Z","shell.execute_reply.started":"2025-12-15T16:09:33.789321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = compute_scores(retain_acts, forget_acts, timesteps=TIMESTEPS, top_k=LAYER_NAV_K)\n\n\nsave_report(results, './layernav.json')","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:09:38.896216Z","iopub.status.busy":"2025-12-15T16:09:38.895682Z","iopub.status.idle":"2025-12-15T16:09:39.346522Z","shell.execute_reply":"2025-12-15T16:09:39.345753Z","shell.execute_reply.started":"2025-12-15T16:09:38.896192Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BEST_LAYERS = list(set([l[0] for value in results.values() for l in value]))\n","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:09:41.526506Z","iopub.status.busy":"2025-12-15T16:09:41.526242Z","iopub.status.idle":"2025-12-15T16:09:41.531908Z","shell.execute_reply":"2025-12-15T16:09:41.531270Z","shell.execute_reply.started":"2025-12-15T16:09:41.526485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def contrastive_pca(X, Y, alpha: float = 1.0, threshold: float = 0.95, min_eigen: float = 0.5):\n    X = X.float()\n    Y = Y.float()\n\n    X_center = X.mean(dim=0, keepdim=True)\n    Xc = X - X_center\n    Yc = Y - Y.mean(dim=0, keepdim=True)\n    \n    N, D = Xc.shape\n    M, _ = Yc.shape\n\n    S_X = (Xc.T @ Xc) / (N - 1)\n    S_Y = (Yc.T @ Yc) / (M - 1)\n\n    S_c = S_X - alpha * S_Y\n\n    eigvals, eigvecs = torch.linalg.eigh(S_c)\n\n    eigvals, idx = torch.sort(eigvals, descending=True)\n    eigvecs = eigvecs[:, idx]\n\n    mask = eigvals > min_eigen\n    eigvals_pos = eigvals[mask]\n    eigvecs_pos = eigvecs[:, mask]\n\n    cum_var = torch.cumsum(eigvals_pos, dim=0) / eigvals_pos.sum()\n    num_components = (cum_var < threshold).sum() + 1\n\n    eigvals_selected = eigvals_pos[:num_components]\n    eigvecs_selected = eigvecs_pos[:, :num_components]\n\n    return eigvecs_selected.cpu(), eigvals_selected.cpu(), X_center.cpu()\n\n\ndef compute_principal_components(forget_acts, retain_acts, layers: list[str], min_subspace_dim: int = 5, alpha: float = 1.0, threshold: float = 0.95, min_eigen: float = 0.5):\n    result = defaultdict(lambda: defaultdict(int))\n    for (layer, X), (_, Y) in zip(forget_acts.items(), retain_acts.items()): \n        X = forget_acts[layer]\n        Y = retain_acts[layer]\n\n        assert X.shape == Y.shape\n\n        for ts in range(X.size(1)):\n            eigvecs, eigvals, mean = contrastive_pca(X[:, ts, :], Y[:, ts, :], alpha, threshold, min_eigen)\n\n            if len(eigvals) >= min_subspace_dim: # we require at least 5 PC  \n                \n                result[layer][ts] = (eigvecs, mean)\n\n    return result\n\nlayer_principal_components = compute_principal_components(\n    forget_acts,\n    retain_acts,\n    layers=BEST_LAYERS,\n    min_subspace_dim=5,\n    alpha=10,\n    threshold=0.90,\n    min_eigen=0.01\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate steering vectors\n\ndef compute_mean_differences(forget_layers_act, retain_layers_act):\n    result = {}\n    for (layer, X), (_, Y) in zip(forget_layers_act.items(), retain_layers_act.items()):\n        result[layer] = (X - Y).mean(dim=0)\n\n    return result\n\nsteering_vectors = compute_mean_differences(forget_acts, retain_acts)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:10:50.942885Z","iopub.status.busy":"2025-12-15T16:10:50.942085Z","iopub.status.idle":"2025-12-15T16:10:50.967631Z","shell.execute_reply":"2025-12-15T16:10:50.966974Z","shell.execute_reply.started":"2025-12-15T16:10:50.942855Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def steer_activations(x, r, lam=-1.0):\n    if torch.all(r == 0).item():\n        return x\n\n    r = r.to(x.device, x.dtype)\n    r /= r.norm()\n        \n    if x.ndim == 4:  # [B, C, H, W]\n        r = r[None, :, None, None] # shape [B, C, 1, 1]\n        channel_dim = 1\n    elif x.ndim == 3: # [B, T, C] (ff layers)\n        r = r[None, None, :] # shape [B, C]\n        channel_dim = 2\n        \n    dot_product = (x * r).sum(dim=channel_dim, keepdim=True)\n    \n    return x + (lam * dot_product * r)\n\n\ndef steer_activations_pca(x, r, pca_data: tuple[torch.Tensor, torch.Tensor], lam: float = -1.0):\n    \n    if torch.all(r == 0).item():\n        return x\n\n    r = r.to(x.device, x.dtype)\n\n    pcs = pca_data[0].half().to(x.device)\n    mean = pca_data[1].half().to(x.device)\n\n    \n\n    x_compressed = (x - mean) @ pcs\n    x_compressed = x @ pcs\n    r_compressed = r @ pcs\n    r_constructed = (r_compressed @ pcs.T) + mean\n    \n    r_compressed /= r_compressed.norm()\n    r_constructed /= r_constructed.norm()\n        \n    if x.ndim == 4:  # [B, C, H, W]\n        r = r[None, :, None, None] # shape [B, C, 1, 1]\n        channel_dim = 1\n    elif x.ndim == 3: # [B, T, C] (attention layers)\n        r = r[None, None, :] # shape [B, C]\n        channel_dim = 2\n        \n    dot_product = (x_compressed * r_compressed).sum(dim=channel_dim, keepdim=True)\n    \n    return x + (lam * dot_product * r_constructed)\n\n\ndef generate_with_steering(\n    pipe: StableDiffusionPipeline,\n    prompt: str,\n    guidance: float,\n    nets: dict,\n    steering_vectors: dict[str, torch.Tensor],\n    timesteps: list[int],\n    inference_steps: int,\n    lam: float,\n    pca_dict: dict[str, dict[int, tuple[torch.Tensor, torch.Tensor]]]\n):\n    handles = []\n\n    current_step = 0\n\n    def steering_hook(layer: str, steering_vector: torch.Tensor):\n        ts_index = 0\n        \n        def hook(module, inp, out):\n            nonlocal ts_index\n            \n\n            # out can be tensor or (hidden, tensor)\n            if isinstance(out, tuple):\n                hidden, activation = out\n            else:\n                hidden, activation = None, out  # activation: [B, C, H, W]\n\n            if current_step in timesteps: \n                \n                B = activation.size(0)\n\n                assert B % 2 == 0\n                \n                x = activation[B // 2 :, :, :]\n\n                pca_data = pca_dict.get(layer, {}).get(current_step, None) if pca_dict is not None else None\n\n                if pca_data is not None:\n                    x_steered = steer_activations_pca(x, steering_vector[ts_index], pca_data, lam)\n                else:\n                    x_steered = steer_activations(x, steering_vector[ts_index], lam)\n                \n                activation[B // 2 :] = x_steered\n                    \n                ts_index += 1\n\n            if hidden is None:\n                return activation\n            else:\n                return (hidden, activation)\n\n        return hook\n\n    for layer, steering_vector in steering_vectors.items():\n        handles.append(\n            nets[layer].register_forward_hook(steering_hook(layer, steering_vector))\n        )\n\n    def callback(pipeline, step_index, timestep, callback_kwargs):\n        nonlocal current_step\n        current_step = step_index\n\n        return callback_kwargs\n    \n    try:\n        return pipe(\n            prompt,\n            num_inference_steps=inference_steps,\n            guidance_scale=guidance,\n            callback_on_step_end=callback,\n            generator=torch.Generator(device=\"cuda\").manual_seed(362)\n        ).images\n    except Exception as e:\n        raise e\n    finally:\n        for h in handles:\n            h.remove()\n\n\n# Run generation with steering\n\nBATCH_SIZE = 16\n\nLAMBDA_PARAMS = [-2.5, 0]\nK_PARAMS = [1, 3, 5, 8, 10]\nTIMESTEP_PARAMS = ['all']\nOUTPUT_DIR = './'\nos.makedirs(f'{OUTPUT_DIR}/base_imgs', exist_ok=True)\nos.makedirs(f'{OUTPUT_DIR}/steered_imgs', exist_ok=True)\n\n\nfor i in range(0, len(eval_dataset), BATCH_SIZE):\n    prompt_batch = eval_dataset[i:i + BATCH_SIZE]\n    print(prompt_batch)\n    for lam in LAMBDA_PARAMS:\n        if lam == 0:\n            images = generate_with_steering(\n                        pipe,\n                        prompt_batch,\n                        GUIDANCE,\n                        nets,\n                        steering_vectors,\n                        timesteps=get_timesteps_by_name(STEPS, 'all'),\n                        inference_steps=STEPS,\n                        lam=lam,\n                        pca_dict=None\n                    )   \n            for idx, image in enumerate(images):\n                filename = f'{OUTPUT_DIR}/base_imgs/{i + idx}.png'\n                image.save(filename)\n        else:\n            for k in K_PARAMS:\n                for t in TIMESTEP_PARAMS:\n                    top_k_per_timestep = get_top_k_layers(results, k=k)\n                    filtered_vectors = mask_vectors_by_top_k(\n                                        steering_vectors, \n                                        timesteps=get_timesteps_by_name(STEPS, t), \n                                        top_k_per_timestep=top_k_per_timestep\n                                    )\n            \n                    steered_images = generate_with_steering(\n                        pipe,\n                        prompt_batch,\n                        GUIDANCE,\n                        nets,\n                        filtered_vectors, # steering_vectors,\n                        timesteps=get_timesteps_by_name(STEPS, t),\n                        inference_steps=STEPS,\n                        lam=lam,\n                        pca_dict=None\n                    )\n    \n                    for idx, image in enumerate(steered_images):\n                        filename = f'{OUTPUT_DIR}/steered_imgs/{i + idx}_lambda={lam}_k={k}_t={t}.png'\n                        image.save(filename)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-15T16:44:22.722518Z","iopub.status.busy":"2025-12-15T16:44:22.722241Z","iopub.status.idle":"2025-12-15T16:46:59.065796Z","shell.execute_reply":"2025-12-15T16:46:59.064636Z","shell.execute_reply.started":"2025-12-15T16:44:22.722495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r output_images.zip base_imgs steered_imgs","metadata":{},"outputs":[],"execution_count":null}]}
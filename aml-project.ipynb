{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensordict -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from typing import Optional\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensordict import TensorDict\n",
    "from collections import defaultdict\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:19:34.809591Z",
     "iopub.status.busy": "2025-12-04T16:19:34.808965Z",
     "iopub.status.idle": "2025-12-04T16:19:37.875836Z",
     "shell.execute_reply": "2025-12-04T16:19:37.875213Z",
     "shell.execute_reply.started": "2025-12-04T16:19:34.809565Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46c8709ccb745508f22b139c9f43d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"Manojb/stable-diffusion-2-1-base\", # \"CompVis/stable-diffusion-v1-4\",\n",
    "    safety_checker=None,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:20:15.416991Z",
     "iopub.status.busy": "2025-12-04T16:20:15.416651Z",
     "iopub.status.idle": "2025-12-04T16:20:15.431596Z",
     "shell.execute_reply": "2025-12-04T16:20:15.430502Z",
     "shell.execute_reply.started": "2025-12-04T16:20:15.416968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collect_residual_streams(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    forget_set: list[str],\n",
    "    retain_set: list[str],\n",
    "    steps: int = 30,\n",
    "    guidance: float = 7.5,\n",
    "    from_timestamp: int = 25\n",
    "):\n",
    "    \n",
    "    forget_acts = []\n",
    "    retain_acts = []\n",
    "\n",
    "    for idx, (forget_prompt, retain_prompt) in enumerate(zip(forget_set, retain_set)):\n",
    "        print(f'[{idx+1}] Extracting acts for forget prompt: {forget_prompt}')\n",
    "        forget_act = get_unet_residual_stream(pipe, forget_prompt, steps, guidance, from_timestamp)\n",
    "\n",
    "        print(f'[{idx+1}] Extracting acts for retain prompt: {retain_prompt}')\n",
    "        retain_act = get_unet_residual_stream(pipe, retain_prompt, steps, guidance, from_timestamp)\n",
    "        \n",
    "        forget_acts.append(forget_act)\n",
    "        retain_acts.append(retain_act)\n",
    "\n",
    "    return forget_acts, retain_acts\n",
    "\n",
    "\n",
    "def get_unet_residual_stream(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    prompt: str,\n",
    "    steps: int = 30,\n",
    "    guidance: float = 7.5,\n",
    "    from_timestamp: int = 25\n",
    "):\n",
    "    # designed to be, using batches would cause coherence issues when collecting acts.    \n",
    "\n",
    "    assert 0 < from_timestamp < steps\n",
    "    \n",
    "    residuals_dict = {}\n",
    "    handles = []\n",
    "\n",
    "    def save_residuals(name):\n",
    "        def hook(module, input, output):\n",
    "            # UNet calculates noise prediction for both conditioned and unconditioned input, so we take the second\n",
    "            residual = output[1] if isinstance(output, tuple) else output\n",
    "            residuals_dict.setdefault(name, []).append(residual[1].detach().cpu())\n",
    "        \n",
    "        return hook\n",
    "\n",
    "    for i, block in enumerate(pipe.unet.down_blocks):\n",
    "        for j, resnet in enumerate(block.resnets):\n",
    "            handles.append(resnet.register_forward_hook(save_residuals(f\"down_block_{i}_resnet_{j}\")))\n",
    "\n",
    "    for j, resnet in enumerate(pipe.unet.mid_block.resnets):\n",
    "        handles.append(resnet.register_forward_hook(save_residuals(f\"mid_block_resnet_{j}\")))\n",
    "\n",
    "    for i, block in enumerate(pipe.unet.up_blocks):\n",
    "        for j, resnet in enumerate(block.resnets):\n",
    "            handles.append(resnet.register_forward_hook(save_residuals(f\"up_block_{i}_resnet_{j}\")))\n",
    "\n",
    "    pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=guidance\n",
    "    )\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    residuals_by_timestep = {\n",
    "        layer: torch.stack(tensors, dim=0)[from_timestamp:]\n",
    "        for layer, tensors in residuals_dict.items()\n",
    "    }\n",
    "\n",
    "    return residuals_by_timestep # [T, C, H, W]\n",
    "\n",
    "def show_images(images: list[Image.Image], prompts: list[str], cols: int = 2, width: int = 40) -> None:\n",
    "    assert len(images) == len(prompts)\n",
    "\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax in axes[len(images):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax, img, prompt in zip(axes, images, prompts):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        wrapped_prompt = \"\\n\".join(textwrap.wrap(prompt, width=width))\n",
    "        ax.text(0.5, -0.05, wrapped_prompt, fontsize=10, ha='center', va='top', transform=ax.transAxes)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_color('black')\n",
    "            spine.set_linewidth(1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_layers_activations_at_timestep(forget_acts, retain_acts, layers: list[str], ts_idx: int):\n",
    "    result = {}\n",
    "\n",
    "    forget_layers = {}\n",
    "    retain_layers = {}\n",
    "    \n",
    "    for l in layers:\n",
    "        forget_layers[l] = torch.stack([f[l][ts_idx] for f in forget_acts], dim=0)\n",
    "        retain_layers[l] = torch.stack([r[l][ts_idx] for r in retain_acts], dim=0)\n",
    "        \n",
    "    return forget_layers, retain_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dogs_dataset = pd.read_csv('/kaggle/input/prompts-steering/dogs.csv')\n",
    "\n",
    "dog_prompts = dogs_dataset['positive'].tolist()\n",
    "non_dog_prompts = dogs_dataset['negative'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a bunch of images just to visualize them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "images_no = 6 # len(dog_prompts)\n",
    "\n",
    "all_images = []\n",
    "all_prompts = []\n",
    "\n",
    "for i in range(0, images_no, batch_size):\n",
    "    pos_batch = dog_prompts[i:i+batch_size]\n",
    "    neg_batch = non_dog_prompts[i:i+batch_size]\n",
    "\n",
    "    pos_images = pipe(pos_batch, num_inference_steps=30, guidance_scale=8).images\n",
    "    neg_images = pipe(neg_batch, num_inference_steps=30, guidance_scale=8).images\n",
    "\n",
    "    for p_img, n_img, p_prompt, n_prompt in zip(pos_images, neg_images, pos_batch, neg_batch):\n",
    "        all_images.append(p_img)\n",
    "        all_images.append(n_img)\n",
    "        all_prompts.append(p_prompt)\n",
    "        all_prompts.append(n_prompt)\n",
    "\n",
    "show_images(all_images, all_prompts, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract raw activations and print layer names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T15:55:09.930223Z",
     "iopub.status.busy": "2025-12-04T15:55:09.929655Z",
     "iopub.status.idle": "2025-12-04T15:55:56.601998Z",
     "shell.execute_reply": "2025-12-04T15:55:56.601250Z",
     "shell.execute_reply.started": "2025-12-04T15:55:09.930198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print(pipe.unet)\n",
    "forget_acts, retain_acts = collect_residual_streams(pipe, dog_prompts[:5], non_dog_prompts[:5], 30, 10, 29)\n",
    "\n",
    "layer_names = set(forget_acts[0].keys()) # take first prompt, same layer names...\n",
    "\n",
    "layer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daniele insert your code here -> Layer Navigator**\n",
    "Layer navigator should return a subset of the layers just printed, possibly a dict with scores for report/debug purposes\n",
    "\n",
    "example: layer_navigator(...) -> ['down_block_2_resnet_1', 'mid_block_resnet_0', 'mid_block_resnet_1', 'up_block_2_resnet_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_steering_vector(retain_acts, forget_acts):\n",
    "    refusal_vec = (forget_acts - retain_acts).mean(dim=0)\n",
    "    return refusal_vec # (D)\n",
    "\n",
    "\n",
    "def process_resnet_acts(acts_list, last_t=1):\n",
    "    layer_names = list(acts_list[0].keys())\n",
    "    aggregated_layers = defaultdict(list)\n",
    "    \n",
    "    for sample in acts_list:\n",
    "        for layer, activation in sample.items():\n",
    "            aggregated_layers[layer].append(activation)\n",
    "    temp_dict = {}\n",
    "\n",
    "    for layer_name, step_list in aggregated_layers.items():\n",
    "        # step_list is a list of Tensor(N, C, H, W) for each timestep\n",
    "\n",
    "        last_steps = step_list[-last_t:] # take last_t steps\n",
    "        stack = torch.stack(step_list, dim=0).to('cuda') # (Steps, N, C, H, W)\n",
    "\n",
    "        time_averaged = stack.mean(dim=0) # (N, C, H, W)\n",
    "        #flattened = time_averaged.flatten(start_dim=1)\n",
    "        spatial_averaged = time_averaged.mean(dim=(2,3))\n",
    "        temp_dict[layer_name] = spatial_averaged.float() # flattened.float()\n",
    "\n",
    "    processed_layers = TensorDict(\n",
    "        temp_dict, \n",
    "        batch_size=[spatial_averaged.shape[0]], # [flattened.shape[0]], \n",
    "        device='cuda'\n",
    "    )\n",
    "    return processed_layers\n",
    "\n",
    "\n",
    "def compute_scores(retain_acts, forget_acts):\n",
    "    results = {}\n",
    "\n",
    "    for layer in layer_names:\n",
    "        P = retain_acts[layer]  # Positive (N, D)\n",
    "        N = forget_acts[layer]  # Negative (N, D)\n",
    "\n",
    "        if P.shape != N.shape:\n",
    "            print(f'P shape and N shape differs in {layer}')\n",
    "\n",
    "        n_samples = P.shape[0]\n",
    "\n",
    "        all_acts = torch.cat([P, N], dim=0) # (2N, D)\n",
    "        mu_l = all_acts.mean(dim=0, keepdim=True)  # (1, D)\n",
    "        sigma_l = all_acts.std(dim=0, keepdim=True) + 1e-8 # (1, D)\n",
    "\n",
    "        P_tilde = (P - mu_l) / sigma_l\n",
    "        N_tilde = (N - mu_l) / sigma_l\n",
    "        \n",
    "        v_l = compute_steering_vector(P, N) # (D)\n",
    "\n",
    "        \n",
    "        # Calculate means of normalized data\n",
    "        mu_pos = P_tilde.mean(dim=0) # (D)\n",
    "        mu_neg = N_tilde.mean(dim=0) # (D)\n",
    "\n",
    "        # Instead of creating (D, D) matrix, project means onto v_l\n",
    "        proj_pos = torch.dot(mu_pos, v_l)\n",
    "        proj_neg = torch.dot(mu_neg, v_l)\n",
    "        \n",
    "        # v^T Sb v = N * (proj_pos^2 + proj_neg^2)\n",
    "        sb_val = n_samples * (proj_pos**2 + proj_neg**2)\n",
    "\n",
    "        # Center the data class-wise\n",
    "        P_centered = P_tilde - mu_pos.unsqueeze(0) # (N, D)\n",
    "        N_centered = N_tilde - mu_neg.unsqueeze(0) # (N, D)\n",
    "\n",
    "        # Instead of creating (D, D) covariance, project data onto v_l\n",
    "        # This calculates the variance of the data along the direction of v_l\n",
    "        p_proj = torch.mv(P_centered, v_l) # (N)\n",
    "        n_proj = torch.mv(N_centered, v_l) # (N)\n",
    "\n",
    "        sw_pos_val = torch.sum(p_proj**2)\n",
    "        sw_neg_val = torch.sum(n_proj**2)\n",
    "        sw_val = sw_pos_val + sw_neg_val\n",
    "\n",
    "        \n",
    "        D_l = (sb_val / (sb_val + sw_val + 1e-8)).item()\n",
    "\n",
    "        pair_diffs = N_tilde - P_tilde # (N, D)\n",
    "        dot_products = torch.mv(pair_diffs, v_l) # (N)\n",
    "        pair_norms = torch.norm(pair_diffs, dim=1) # (N,)\n",
    "        v_norm = torch.norm(v_l)\n",
    "        \n",
    "        cosine_sims = dot_products / (pair_norms * v_norm + 1e-8)\n",
    "        C_l = cosine_sims.mean().item()\n",
    "\n",
    "        S_l = D_l + C_l\n",
    "\n",
    "        results[layer] = {\n",
    "            \"score\": S_l,\n",
    "            \"discriminability\": D_l,\n",
    "            \"consistency\": C_l\n",
    "        }\n",
    "        \n",
    "        del P_tilde, N_tilde, all_acts, P_centered, N_centered\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_top_k_layers(results, k=5):\n",
    "    sorted_layers = sorted(results.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    return [x[0] for x in sorted_layers[:k]]\n",
    "\n",
    "def print_report(results, top_n=5):\n",
    "    print(f\"\\n{'Layer Name':<40} | {'Score':<8} | {'Discrim':<8} | {'Consist':<8}\")\n",
    "    print(\"-\" * 75)\n",
    "    sorted_layers = sorted(results.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    for layer, stats in sorted_layers[:top_n]:\n",
    "        print(f\"{layer:<40} | {stats['score']:.4f}   | {stats['discriminability']:.4f}   | {stats['consistency']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_retain = process_resnet_acts(retain_acts, last_t=1)\n",
    "processed_forget = process_resnet_acts(forget_acts, last_t=1)\n",
    "\n",
    "results = compute_scores(processed_retain, processed_forget)\n",
    "\n",
    "top_k = get_top_k_layers(results, k=3)\n",
    "print_report(results, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:20:31.125742Z",
     "iopub.status.busy": "2025-12-04T16:20:31.125178Z",
     "iopub.status.idle": "2025-12-04T16:20:31.135019Z",
     "shell.execute_reply": "2025-12-04T16:20:31.134360Z",
     "shell.execute_reply.started": "2025-12-04T16:20:31.125715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get timestep specific activations for each specified layer\n",
    "\n",
    "forget_layers_act, retain_layers_act = get_layers_activations_at_timestep(\n",
    "    forget_acts, \n",
    "    retain_acts, \n",
    "    ['down_block_2_resnet_1', 'mid_block_resnet_0', 'mid_block_resnet_1', 'up_block_2_resnet_2'],\n",
    "    -1 # we take the last timestep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:20:33.017362Z",
     "iopub.status.busy": "2025-12-04T16:20:33.016790Z",
     "iopub.status.idle": "2025-12-04T16:20:33.026268Z",
     "shell.execute_reply": "2025-12-04T16:20:33.025512Z",
     "shell.execute_reply.started": "2025-12-04T16:20:33.017336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mean_difference(X, Y, normalize=True):\n",
    "    X_mean = X.mean(dim=0) # [C, H, W]\n",
    "    Y_mean = Y.mean(dim=0) # [C, H, W]\n",
    "\n",
    "    v = X_mean - Y_mean\n",
    "\n",
    "    if normalize:\n",
    "        v = v / v.norm()\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "def compute_mean_differences(forget_layers_act, retain_layers_act, normalize=True):\n",
    "    result = {}\n",
    "    for (layer, f), (_, r) in zip(forget_layers_act.items(), retain_layers_act.items()):\n",
    "        result[layer] = mean_difference(f, r, normalize)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def contrastive_pca(X, Y, n_components=300, alpha=0.8):\n",
    "    B, C, H, W = X.shape\n",
    "    D = C * H * W\n",
    "    \n",
    "    X_flat = X.float().reshape(X.shape[0], -1).to('cuda') # (Bx, D)\n",
    "    Y_flat = Y.float().reshape(Y.shape[0], -1).to('cuda') # (By, D)\n",
    "    \n",
    "    mean_X = X_flat.mean(dim=0, keepdim=True)\n",
    "    mean_Y = Y_flat.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    X_centered = X_flat - mean_X\n",
    "    Y_centered = Y_flat - mean_Y\n",
    "\n",
    "    U, S, Vh = torch.linalg.svd(X_centered, full_matrices=False)\n",
    "    V = Vh.T\n",
    "    \n",
    "    X_proj = U @ torch.diag(S)\n",
    "    Y_proj = Y_centered @ V\n",
    "    \n",
    "    Cx_small = (X_proj.T @ X_proj) / (X.shape[0] - 1)\n",
    "    Cy_small = (Y_proj.T @ Y_proj) / (Y.shape[0] - 1)\n",
    "    \n",
    "    C_dual = Cx_small - alpha * Cy_small\n",
    "    \n",
    "    eigvals, eigvecs_small = torch.linalg.eigh(C_dual)\n",
    "    \n",
    "    idx = torch.argsort(eigvals, descending=True)[:n_components]\n",
    "    top_vecs_small = eigvecs_small[:, idx]\n",
    "    \n",
    "    components = V @ top_vecs_small\n",
    "    \n",
    "    components = components / components.norm(dim=0, keepdim=True)\n",
    "    \n",
    "    return components.T\n",
    "\n",
    "def compute_principal_componets(forget_layers_act, retain_layers_act, n_components=10, alpha=1e-3, whiten=False):\n",
    "    result = {}\n",
    "    for (layer, f), (_, r) in zip(forget_layers_act.items(), retain_layers_act.items()):\n",
    "        result[layer] = contrastive_pca(f, r, n_components, alpha, whiten)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:14:08.880066Z",
     "iopub.status.busy": "2025-12-04T16:14:08.879366Z",
     "iopub.status.idle": "2025-12-04T16:14:08.883390Z",
     "shell.execute_reply": "2025-12-04T16:14:08.882726Z",
     "shell.execute_reply.started": "2025-12-04T16:14:08.880037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# THE PROBLEM IS THAT WITH PCA we have N <<< D\n",
    "# Specifying 300 components to extract but passing a batch of 5-10-30 samples does not make many sense\n",
    "\n",
    "# components = contrastive_pca(\n",
    "#     forget_layers_act['down_block_2_resnet_1'],\n",
    "#     retain_layers_act['down_block_2_resnet_1'],\n",
    "#     300,\n",
    "#     0.8\n",
    "# )\n",
    "\n",
    "# components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:22:15.037623Z",
     "iopub.status.busy": "2025-12-04T16:22:15.036930Z",
     "iopub.status.idle": "2025-12-04T16:22:15.057448Z",
     "shell.execute_reply": "2025-12-04T16:22:15.056805Z",
     "shell.execute_reply.started": "2025-12-04T16:22:15.037596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('down_block_2_resnet_1', torch.Size([1280, 16, 16])), ('mid_block_resnet_0', torch.Size([1280, 8, 8])), ('mid_block_resnet_1', torch.Size([1280, 8, 8])), ('up_block_2_resnet_2', torch.Size([640, 32, 32]))]\n"
     ]
    }
   ],
   "source": [
    "mean_differences = compute_mean_differences(forget_layers_act, retain_layers_act, True)\n",
    "\n",
    "print([(layer, steering_vector.shape) for (layer, steering_vector) in mean_differences.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OLJA, inject you inference code here, mean differences contain the steering vector for each layer specified above**\n",
    "First try simple mean difference, then we can try to use PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# olja, code"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8901347,
     "sourceId": 13963606,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

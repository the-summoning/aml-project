{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14095498,"sourceType":"datasetVersion","datasetId":8976063}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport math\n\nimport textwrap\n\nfrom typing import Optional\nfrom diffusers import StableDiffusionPipeline\nfrom huggingface_hub import notebook_login\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom collections import defaultdict\n\nnotebook_login()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", # \"Manojb/stable-diffusion-2-1-base\" # \"CompVis/stable-diffusion-v1-4\",\n    safety_checker=None,\n    torch_dtype=torch.float16\n).to(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_unet_resnets(pipe):\n    resnets = {}\n    \n    for i, block in enumerate(pipe.unet.down_blocks):\n        for j, resnet in enumerate(block.resnets):\n            resnets[f\"down_block_{i}_resnet_{j}\"] = resnet\n\n    for j, resnet in enumerate(pipe.unet.mid_block.resnets):\n        resnets[f\"mid_block_resnet_{j}\"] = resnet\n\n    for i, block in enumerate(pipe.unet.up_blocks):\n        for j, resnet in enumerate(block.resnets):\n            resnets[f\"up_block_{i}_resnet_{j}\"] = resnet\n\n    return resnets\n\ndef get_unet_trans_ff(pipe):\n    ff_nets = {}\n\n    for i, block in enumerate(pipe.unet.down_blocks):\n        if hasattr(block, \"attentions\"):\n            for j, attn in enumerate(block.attentions):\n                for k, transformer in enumerate(attn.transformer_blocks):\n                    name = f\"down_block_{i}_attn_{j}_trans_{k}_ff\"\n                    ff_nets[name] = transformer.ff\n                    \n    if hasattr(pipe.unet.mid_block, \"attentions\"):\n        for j, attn in enumerate(pipe.unet.mid_block.attentions):\n            for k, transformer in enumerate(attn.transformer_blocks):\n                name = f\"mid_block_attn_{j}_trans_{k}_ff\"\n                ff_nets[name] = transformer.ff\n                \n    for i, block in enumerate(pipe.unet.up_blocks):\n        if hasattr(block, \"attentions\"):\n            for j, attn in enumerate(block.attentions):\n                for k, transformer in enumerate(attn.transformer_blocks):\n                    name = f\"up_block_{i}_attn_{j}_trans_{k}_ff\"\n                    ff_nets[name] = transformer.ff\n\n    return ff_nets\n\nresnets = get_unet_resnets(pipe)\nff_nets = get_unet_trans_ff(pipe)\nprint(list(resnets.keys()))\nprint(list(ff_nets.keys()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dogs_dataset = pd.read_csv('/kaggle/input/new-nudity-steering/nudity.csv')\n\ndog_prompts = dogs_dataset['positive'].tolist()\nnon_dog_prompts = dogs_dataset['negative'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef collect_residual_streams(\n    pipe: StableDiffusionPipeline,\n    forget_set: list[str],\n    retain_set: list[str],\n    guidance: float,\n    resnets: dict,\n    layers: list[str],\n    timesteps: list[int]\n):\n    forget_acts = []\n    retain_acts = []\n\n    for idx, (forget_prompt, retain_prompt) in enumerate(zip(forget_set, retain_set)):\n        print(f'[{idx+1}] Extracting acts for forget prompt: {forget_prompt}')\n        forget_act = get_unet_residual_stream(pipe, forget_prompt, guidance, resnets, layers, timesteps)\n\n        print(f'[{idx+1}] Extracting acts for retain prompt: {retain_prompt}')\n        retain_act = get_unet_residual_stream(pipe, retain_prompt, guidance, resnets, layers, timesteps)\n        \n        forget_acts.append(forget_act)\n        retain_acts.append(retain_act)\n\n    forget_layers = {}\n    retain_layers = {}\n    \n    for l in layers:\n        forget_layers[l] = torch.stack([f[l] for f in forget_acts], dim=0)\n        retain_layers[l] = torch.stack([r[l] for r in retain_acts], dim=0)\n        \n    return forget_layers, retain_layers\n\n\n\ndef get_unet_residual_stream(\n    pipe: StableDiffusionPipeline,\n    prompt: str,\n    guidance: float,\n    resnets: dict,\n    layers: list[str],\n    timesteps: list[int]\n):\n    # designed to be simple, using batches would cause coherence issues when collecting acts.\n    residuals_dict = {}\n    handles = []\n\n    current_step = 0\n\n    def save_residuals(name):\n        def hook(module, input, output):           \n            if current_step in timesteps:\n                # UNet calculates noise prediction for both conditioned and unconditioned input, so we take the second\n                residual = output[1] if isinstance(output, tuple) else output\n                residuals_dict.setdefault(name, []).append(residual[1].detach().cpu())\n\n        return hook\n\n    for l in layers:\n        handles.append(\n            resnets[l].register_forward_hook(save_residuals(l))\n        )\n\n    def callback(pipeline, step_index, timestep, callback_kwargs):\n        nonlocal current_step\n        current_step = step_index\n\n        return callback_kwargs\n    \n    try:\n        images = pipe(\n            prompt,\n            num_inference_steps=timesteps[-1],\n            guidance_scale=guidance,\n            callback_on_step_end=callback\n        )\n        \n        return {\n            layer: torch.stack(tensors, dim=0)\n            for layer, tensors in residuals_dict.items()\n        } # [T, C, H, W]\n    except Exception as e:\n        raise e\n    finally:\n        for h in handles:\n            h.remove()\n\ndef show_images(images: list[Image.Image], prompts: list[str], cols: int = 2, width: int = 40) -> None:\n    assert len(images) == len(prompts)\n\n    rows = math.ceil(len(images) / cols)\n    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n\n    if isinstance(axes, np.ndarray):\n        axes = axes.flatten()\n    else:\n        axes = [axes]\n\n    for ax in axes[len(images):]:\n        ax.axis('off')\n\n    for ax, img, prompt in zip(axes, images, prompts):\n        ax.imshow(img)\n        ax.axis('off')\n        wrapped_prompt = \"\\n\".join(textwrap.wrap(prompt, width=width))\n        ax.text(0.5, -0.05, wrapped_prompt, fontsize=10, ha='center', va='top', transform=ax.transAxes)\n        for spine in ax.spines.values():\n            spine.set_visible(True)\n            spine.set_color('black')\n            spine.set_linewidth(1)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extract raw activations and print layer names**","metadata":{}},{"cell_type":"code","source":"#print(pipe.unet)\nALL_LAYERS = list((resnets | ff_nets).keys()) # list(resnets.keys())\nGUIDANCE = 7.5\nLAYERS = ALL_LAYERS\nSTEPS = 30\nTIMESTEPS = list(range(1, STEPS+1))\nforget_acts, retain_acts = collect_residual_streams(\n    pipe,\n    dog_prompts[:5],\n    non_dog_prompts[:5],\n    guidance=GUIDANCE,\n    resnets=(resnets | ff_nets),\n    layers=LAYERS,\n    timesteps=TIMESTEPS\n)\n\nfor layer, act in forget_acts.items():\n    print(f'Layer {layer}: {act.shape}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_mean_differences(forget_layers_act, retain_layers_act, layer_nav=False):\n    if layer_nav:\n        return (forget_layers_act - retain_layers_act).mean(dim=0)\n        \n    result = {}\n    for (layer, X), (_, Y) in zip(forget_layers_act.items(), retain_layers_act.items()):\n        if X.ndim == 5: # Resnet activation\n            result[layer] = (X - Y).mean(dim=(0, 3, 4)) # [T, C]\n        else: # FF net activation\n            result[layer] = (X - Y).mean(dim=(0, 2))\n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_acts(acts_dict):\n    temp_dict = {}\n    for layer_name, act in acts_dict.items():\n        # act is a  Tensor(N, steps, C, H, W) for each item in the batch\n\n        act = act.to('cuda') # (N, Steps, C, H, W)\n\n        if act.ndim == 5: # Resnet activation\n            spatial_averaged = act.mean(dim=(3,4)) # (N, steps, C)\n        else: # Feed forward attention activation\n            spatial_averaged = act.mean(dim=2) # (N, steps, C)\n            \n        temp_dict[layer_name] = spatial_averaged.float() \n\n    \n    return temp_dict\n\n\ndef compute_scores(retain_acts, forget_acts, timesteps, top_k):\n    results = {}\n    retain_acts, forget_acts = process_acts(retain_acts), process_acts(forget_acts)\n\n    for timestep in timesteps:\n        timestep_dict = {}\n        for layer in retain_acts:\n            P = retain_acts[layer][:, timestep-timesteps[0], :]  # Positive (N, D)\n            N = forget_acts[layer][:, timestep-timesteps[0], :]  # Negative (N, D)\n    \n            if P.shape != N.shape:\n                print(f'P shape and N shape differs in {layer}')\n    \n            n_samples = P.shape[0]\n    \n            all_acts = torch.cat([P, N], dim=0) # (2N, D)\n            mu_l = all_acts.mean(dim=0, keepdim=True)  # (1, D)\n            sigma_l = all_acts.std(dim=0, keepdim=True) + 1e-8 # (1, D)\n    \n            P_tilde = (P - mu_l) / sigma_l\n            N_tilde = (N - mu_l) / sigma_l\n            \n            v_l = compute_mean_differences(N, P, layer_nav=True) # (D)\n    \n            \n            # Calculate means of normalized data\n            mu_pos = P_tilde.mean(dim=0) # (D)\n            mu_neg = N_tilde.mean(dim=0) # (D)\n    \n            # Instead of creating (D, D) matrix, project means onto v_l\n            proj_pos = torch.dot(mu_pos, v_l)\n            proj_neg = torch.dot(mu_neg, v_l)\n            \n            # v^T Sb v = N * (proj_pos^2 + proj_neg^2)\n            sb_val = n_samples * (proj_pos**2 + proj_neg**2)\n    \n            # Center the data class-wise\n            P_centered = P_tilde - mu_pos.unsqueeze(0) # (N, D)\n            N_centered = N_tilde - mu_neg.unsqueeze(0) # (N, D)\n    \n            # Instead of creating (D, D) covariance, project data onto v_l\n            # This calculates the variance of the data along the direction of v_l\n            p_proj = torch.mv(P_centered, v_l) # (N)\n            n_proj = torch.mv(N_centered, v_l) # (N)\n    \n            sw_pos_val = torch.sum(p_proj**2)\n            sw_neg_val = torch.sum(n_proj**2)\n            sw_val = sw_pos_val + sw_neg_val\n    \n            \n            D_l = (sb_val / (sb_val + sw_val + 1e-8)).item()\n    \n            pair_diffs = N_tilde - P_tilde # (N, D)\n            dot_products = torch.mv(pair_diffs, v_l) # (N)\n            pair_norms = torch.norm(pair_diffs, dim=1) # (N,)\n            v_norm = torch.norm(v_l)\n            \n            cosine_sims = dot_products / (pair_norms * v_norm + 1e-8)\n            C_l = cosine_sims.mean().item()\n    \n            S_l = D_l + C_l\n    \n            timestep_dict[layer] = {\n                \"score\": S_l,\n                \"discriminability\": D_l,\n                \"consistency\": C_l\n            }\n            \n            del P_tilde, N_tilde, all_acts, P_centered, N_centered\n            torch.cuda.empty_cache()\n        sorted_layers = sorted(timestep_dict.items(), key=lambda x: x[1]['score'], reverse=True)\n        results[timestep] = [x for x in sorted_layers[:top_k]]\n    return results\n\ndef get_top_k_layers(results):\n    res = {}\n    for timestep, top in results.items():\n        res[timestep] = [x[0] for x in top]\n    return res\n\ndef print_report(results):\n    for timestep, top in results.items():\n        print(f\"Timestep: {timestep}\")\n        for layer, score_dict in top:\n            score, disc, cons = score_dict['score'], score_dict['discriminability'], score_dict['consistency']\n            print(f'\\tLayer: {layer} | Score: {score} | Disc: {disc} | Cons: {cons}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = compute_scores(retain_acts, forget_acts, timesteps=TIMESTEPS[:-1], top_k=5)\n\ntop_k_per_timestep = get_top_k_layers(results)\nprint_report(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate steering vectors\nsteering_vectors = compute_mean_differences(forget_acts, retain_acts)\n\nprint([(layer, steering_vector.shape) for (layer, steering_vector) in steering_vectors.items()])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def steer_activations(x, r, lam=-1.0):\n    #print(f'Steering on {x.shape} with {r.shape}')\n    \n    r = r.to(x.device, x.dtype)\n    r = r/r.norm()\n    if x.ndim == 3:  # [C, H, W]\n        r = r[:, None, None]      # shape [C, 1, 1]\n        channel_dim = 0\n    elif x.ndim == 4:  # [1, C, H, W]\n        r = r[None, :, None, None] # shape [1, C, 1, 1]\n        channel_dim = 1\n    else: # [S, C] (ff layers)\n        r = r[None, :] # shape [1, C]\n        channel_dim = 1\n        \n    \n    dot_product = (x * r).sum(dim=channel_dim, keepdim=True)\n    \n    return x + lam * (dot_product * r)\n\n\ndef generate_with_steering(\n    pipe: StableDiffusionPipeline,\n    prompt: str,\n    guidance: float,\n    resnets: dict,\n    steering_vectors: dict[str, torch.Tensor],\n    timesteps: list[int],\n    lam: float,\n    top_layers_per_timestep: dict[int, list[str]]\n):\n    # designed to be simple, using batches would cause coherence issues when collecting acts.\n    residuals_dict = {}\n    handles = []\n\n    current_step = 0\n\n    def steering_hook(layer: str, steering_vector: torch.Tensor):\n        ts_index = 0\n        \n        def hook(module, inp, out):\n            nonlocal ts_index\n            #print(f\"[STEERING] layer={layer_name} step={current_step}\")\n\n            # out can be tensor or (hidden, tensor)\n            if isinstance(out, tuple):\n                hidden, residual = out\n            else:\n                hidden, residual = None, out  # residual: [B, C, H, W]\n                \n            if current_step in timesteps:\n                # If a dict with top layers for each timestep has been passed use that\n                if isinstance(top_layers_per_timestep, dict): \n                    current_top_layers = top_layers_per_timestep[current_step]\n                else: # Otherwise assume ALL_LAYERS has been passed\n                    current_top_layers = top_layers_per_timestep\n                    \n                if layer in current_top_layers:\n                    #print(f'[{layer}] -> Step {current_step}, ts_index {ts_index}')\n                    \n                    x = residual[1]\n                    x_steered = steer_activations(x, steering_vector[ts_index], lam)\n                    residual[1] = x_steered\n                    \n                ts_index += 1\n\n            if hidden is None:\n                return residual\n            else:\n                return (hidden, residual)\n\n        return hook\n\n    for layer, steering_vector in steering_vectors.items():\n        handles.append(\n            resnets[layer].register_forward_hook(steering_hook(layer, steering_vector))\n        )\n\n    def callback(pipeline, step_index, timestep, callback_kwargs):\n        nonlocal current_step\n        current_step = step_index\n\n        return callback_kwargs\n    \n    try:\n        return pipe(\n            prompt,\n            num_inference_steps=timesteps[-1],\n            guidance_scale=guidance,\n            callback_on_step_end=callback,\n            generator=torch.Generator(device=\"cuda\").manual_seed(362)\n        ).images\n    except Exception as e:\n        raise e\n    finally:\n        for h in handles:\n            h.remove()\n\n\n# Run generation with steering\nprompt = dog_prompts[1]\n\nall_images = []\nlambdas = []\nprint(prompt)\nfor lam in torch.arange(-3, 3, 0.5):\n    steered_images = generate_with_steering(\n        pipe,\n        prompt,\n        GUIDANCE,\n        (resnets | ff_nets),\n        steering_vectors,\n        timesteps=TIMESTEPS,\n        lam=lam,\n        top_layers_per_timestep= top_k_per_timestep # ALL_LAYERS\n    )\n    all_images.extend(steered_images)\n    lambdas.append(str(lam.item()))\n\n# Visualize\nshow_images(all_images, lambdas, cols=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13993346,"sourceType":"datasetVersion","datasetId":8918031}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"#Optional: install library extra \n!pip install open-clip-torch  # for CLIP \n!pip install pillow scipy tqdm\n!pip install openai           # for GPT score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:30:46.092447Z","iopub.execute_input":"2025-12-04T16:30:46.092826Z","iopub.status.idle":"2025-12-04T16:33:25.871219Z","shell.execute_reply.started":"2025-12-04T16:30:46.092800Z","shell.execute_reply":"2025-12-04T16:33:25.870405Z"}},"outputs":[{"name":"stdout","text":"Collecting open-clip-torch\n  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.21.0+cu124)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2025.11.3)\nCollecting ftfy (from open-clip-torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.3)\nRequirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.19)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm>=1.0.17->open-clip-torch) (6.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->open-clip-torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->open-clip-torch) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (1.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->open-clip-torch) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open-clip-torch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->open-clip-torch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\nDownloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, open-clip-torch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 open-clip-torch-3.2.0\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\nRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (2.7.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.11.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.12.4)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.15.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# For FID (InceptionV3)\nfrom torchvision import models, transforms\n\n# For CLIP \nimport open_clip\n\n# For GPT score\nfrom openai import OpenAI  # <-- nuova API\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:25.873433Z","iopub.execute_input":"2025-12-04T16:33:25.873678Z","iopub.status.idle":"2025-12-04T16:33:41.167965Z","shell.execute_reply.started":"2025-12-04T16:33:25.873654Z","shell.execute_reply":"2025-12-04T16:33:41.167355Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Config generals\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", DEVICE)\n\n# unknown directories\nBASELINE_DIR = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/baseline_test\")  # TODO: aggiorna percorso\nSTEERED_DIR  = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/steered_test\")   # TODO: aggiorna percorso\n\n# file with prompts \nPROMPTS_JSON = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/prompts.json\")  \n\n# OpenAI client for GPT score \n#client = OpenAI()  # use OPENAI_API_KEY from sistema\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:41.168650Z","iopub.execute_input":"2025-12-04T16:33:41.168878Z","iopub.status.idle":"2025-12-04T16:33:41.254651Z","shell.execute_reply.started":"2025-12-04T16:33:41.168860Z","shell.execute_reply":"2025-12-04T16:33:41.253850Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Load Images**","metadata":{}},{"cell_type":"code","source":"IMAGE_EXTS = [\".png\", \".jpg\", \".jpeg\", \".webp\"]\n\ndef list_images(folder: Path):\n    return sorted([\n        p for p in folder.iterdir() \n        if p.suffix.lower() in IMAGE_EXTS\n    ])\n\ndef load_pil_image(path: Path):\n    return Image.open(path).convert(\"RGB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:41.255493Z","iopub.execute_input":"2025-12-04T16:33:41.255799Z","iopub.status.idle":"2025-12-04T16:33:41.274764Z","shell.execute_reply.started":"2025-12-04T16:33:41.255770Z","shell.execute_reply":"2025-12-04T16:33:41.274075Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# STandard transformations\neval_transform = transforms.Compose([\n    transforms.Resize((299, 299)),  # for InceptionV3 (FID)\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std =[0.229, 0.224, 0.225],\n    ),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:41.275460Z","iopub.execute_input":"2025-12-04T16:33:41.275720Z","iopub.status.idle":"2025-12-04T16:33:41.288882Z","shell.execute_reply.started":"2025-12-04T16:33:41.275696Z","shell.execute_reply":"2025-12-04T16:33:41.288195Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**FID**","metadata":{}},{"cell_type":"code","source":"class InceptionFID(nn.Module):\n    \"\"\"\n    extract from InceptionV3 (pool3) to calculate FID.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        inception = models.inception_v3(\n            weights=models.Inception_V3_Weights.IMAGENET1K_V1,\n            transform_input=False\n        )\n        inception.fc = nn.Identity()  # togliamo il classificatore\n        inception.eval()\n        self.inception = inception.to(DEVICE)\n    \n    @torch.no_grad()\n    def forward(self, x):\n        # x: (B,3,299,299)\n        return self.inception(x)  # (B, 2048) tipicamente\n        \n\nfid_model = InceptionFID()\nfid_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:41.291141Z","iopub.execute_input":"2025-12-04T16:33:41.291342Z","iopub.status.idle":"2025-12-04T16:33:42.539034Z","shell.execute_reply.started":"2025-12-04T16:33:41.291326Z","shell.execute_reply":"2025-12-04T16:33:42.538386Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 205MB/s]  \n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"InceptionFID(\n  (inception): Inception3(\n    (Conv2d_1a_3x3): BasicConv2d(\n      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (Conv2d_2a_3x3): BasicConv2d(\n      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (Conv2d_2b_3x3): BasicConv2d(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (Conv2d_3b_1x1): BasicConv2d(\n      (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (Conv2d_4a_3x3): BasicConv2d(\n      (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (Mixed_5b): InceptionA(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch5x5_1): BasicConv2d(\n        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch5x5_2): BasicConv2d(\n        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_1): BasicConv2d(\n        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_2): BasicConv2d(\n        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_3): BasicConv2d(\n        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_5c): InceptionA(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch5x5_1): BasicConv2d(\n        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch5x5_2): BasicConv2d(\n        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_1): BasicConv2d(\n        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_2): BasicConv2d(\n        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_3): BasicConv2d(\n        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_5d): InceptionA(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch5x5_1): BasicConv2d(\n        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch5x5_2): BasicConv2d(\n        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_1): BasicConv2d(\n        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_2): BasicConv2d(\n        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_3): BasicConv2d(\n        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_6a): InceptionB(\n      (branch3x3): BasicConv2d(\n        (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_1): BasicConv2d(\n        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_2): BasicConv2d(\n        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_3): BasicConv2d(\n        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_6b): InceptionC(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_1): BasicConv2d(\n        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_2): BasicConv2d(\n        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_3): BasicConv2d(\n        (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_1): BasicConv2d(\n        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_2): BasicConv2d(\n        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_3): BasicConv2d(\n        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_4): BasicConv2d(\n        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_5): BasicConv2d(\n        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_6c): InceptionC(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_1): BasicConv2d(\n        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_2): BasicConv2d(\n        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_3): BasicConv2d(\n        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_1): BasicConv2d(\n        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_2): BasicConv2d(\n        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_3): BasicConv2d(\n        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_4): BasicConv2d(\n        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_5): BasicConv2d(\n        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_6d): InceptionC(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_1): BasicConv2d(\n        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_2): BasicConv2d(\n        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_3): BasicConv2d(\n        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_1): BasicConv2d(\n        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_2): BasicConv2d(\n        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_3): BasicConv2d(\n        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_4): BasicConv2d(\n        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_5): BasicConv2d(\n        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_6e): InceptionC(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_1): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_2): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7_3): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_1): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_2): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_3): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_4): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7dbl_5): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (AuxLogits): InceptionAux(\n      (conv0): BasicConv2d(\n        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): BasicConv2d(\n        (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (fc): Linear(in_features=768, out_features=1000, bias=True)\n    )\n    (Mixed_7a): InceptionD(\n      (branch3x3_1): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3_2): BasicConv2d(\n        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7x3_1): BasicConv2d(\n        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7x3_2): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7x3_3): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch7x7x3_4): BasicConv2d(\n        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_7b): InceptionE(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3_1): BasicConv2d(\n        (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3_2a): BasicConv2d(\n        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3_2b): BasicConv2d(\n        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_1): BasicConv2d(\n        (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_2): BasicConv2d(\n        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_3a): BasicConv2d(\n        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_3b): BasicConv2d(\n        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (Mixed_7c): InceptionE(\n      (branch1x1): BasicConv2d(\n        (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3_1): BasicConv2d(\n        (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3_2a): BasicConv2d(\n        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3_2b): BasicConv2d(\n        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_1): BasicConv2d(\n        (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_2): BasicConv2d(\n        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_3a): BasicConv2d(\n        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch3x3dbl_3b): BasicConv2d(\n        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (branch_pool): BasicConv2d(\n        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (dropout): Dropout(p=0.5, inplace=False)\n    (fc): Identity()\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"**helper FID**","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef get_activations(image_paths, batch_size=32):\n    \"\"\"\n    image_paths: list of Path\n    ritorna: np.array (N, D) with inception features\n    \"\"\"\n    acts = []\n    for i in range(0, len(image_paths), batch_size):\n        batch_paths = image_paths[i:i+batch_size]\n        batch_imgs = []\n        for p in batch_paths:\n            img = load_pil_image(p)\n            img = eval_transform(img)\n            batch_imgs.append(img)\n        batch = torch.stack(batch_imgs, dim=0).to(DEVICE)\n        feats = fid_model(batch)\n        acts.append(feats.cpu().numpy())\n    acts = np.concatenate(acts, axis=0)\n    return acts\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"\n    Fid formulas.\n    \"\"\"\n    from scipy.linalg import sqrtm\n\n    diff = mu1 - mu2\n    covmean, _ = np.linalg.eigh(sigma1 @ sigma2)\n    # Or:\n    # covmean = sqrtm(sigma1.dot(sigma2))\n   \n\n    # Con eigenvalues:\n    covmean = np.sqrt(np.clip(covmean, a_min=0, a_max=None))\n    covmean = np.diag(covmean)\n\n    tr_covmean = np.trace(covmean)\n\n    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n    return float(fid)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:42.539715Z","iopub.execute_input":"2025-12-04T16:33:42.539950Z","iopub.status.idle":"2025-12-04T16:33:42.546279Z","shell.execute_reply.started":"2025-12-04T16:33:42.539917Z","shell.execute_reply":"2025-12-04T16:33:42.545425Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_fid(real_dir: Path, gen_dir: Path, batch_size: int = 32) -> float:\n    \"\"\"\n    Calculate FID between real images (baseline) e generated (steered).\n    \"\"\"\n    real_paths = list_images(real_dir)\n    gen_paths  = list_images(gen_dir)\n\n    assert len(real_paths) == len(gen_paths), \"We assume same number lenght.\"\n\n    real_acts = get_activations(real_paths, batch_size=batch_size)\n    gen_acts  = get_activations(gen_paths,  batch_size=batch_size)\n\n    mu_real = np.mean(real_acts, axis=0)\n    sigma_real = np.cov(real_acts, rowvar=False)\n\n    mu_gen = np.mean(gen_acts, axis=0)\n    sigma_gen = np.cov(gen_acts, rowvar=False)\n\n    fid_value = calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)\n    return fid_value\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:42.546893Z","iopub.execute_input":"2025-12-04T16:33:42.547160Z","iopub.status.idle":"2025-12-04T16:33:42.559268Z","shell.execute_reply.started":"2025-12-04T16:33:42.547137Z","shell.execute_reply":"2025-12-04T16:33:42.558711Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**CLIP**","metadata":{}},{"cell_type":"code","source":"# possible model open_clip;\nclip_model_name = \"ViT-B-32\"\nclip_pretrained  = \"laion2b_s34b_b79k\"\n\nclip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n    clip_model_name, \n    pretrained=clip_pretrained, \n    device=DEVICE\n)\n\nclip_tokenizer = open_clip.get_tokenizer(clip_model_name)\nclip_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:42.559889Z","iopub.execute_input":"2025-12-04T16:33:42.560122Z","iopub.status.idle":"2025-12-04T16:33:46.906917Z","shell.execute_reply.started":"2025-12-04T16:33:42.560106Z","shell.execute_reply":"2025-12-04T16:33:46.906259Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f36671fab294fbf9ae9c4a93c342be2"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CLIP(\n  (visual): VisionTransformer(\n    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n    (patch_dropout): Identity()\n    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (transformer): Transformer(\n      (resblocks): ModuleList(\n        (0-11): 12 x ResidualAttentionBlock(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ls_1): Identity()\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): GELU(approximate='none')\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ls_2): Identity()\n        )\n      )\n    )\n    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (transformer): Transformer(\n    (resblocks): ModuleList(\n      (0-11): 12 x ResidualAttentionBlock(\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ls_1): Identity()\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): GELU(approximate='none')\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ls_2): Identity()\n      )\n    )\n  )\n  (token_embedding): Embedding(49408, 512)\n  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"@torch.no_grad()\ndef compute_clip_score(image_paths, texts):\n    \"\"\"\n    image_paths: list Path\n    texts: list strings (same length) or a single string\n    return: float in [0,1]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts] * len(image_paths)\n    assert len(image_paths) == len(texts)\n\n    all_sims = []\n\n    for p, t in tqdm(list(zip(image_paths, texts)), total=len(image_paths)):\n        img = load_pil_image(p)\n        img = clip_preprocess(img).unsqueeze(0).to(DEVICE)\n\n        tok = clip_tokenizer([t]).to(DEVICE)\n\n        img_feat = clip_model.encode_image(img)\n        txt_feat = clip_model.encode_text(tok)\n\n        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n        txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n\n        cos_sim = (img_feat * txt_feat).sum(dim=-1).item()  # [-1,1]\n        score_01 = (cos_sim + 1) / 2.0\n        all_sims.append(score_01)\n\n    return float(np.mean(all_sims))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:46.907684Z","iopub.execute_input":"2025-12-04T16:33:46.907962Z","iopub.status.idle":"2025-12-04T16:33:46.914936Z","shell.execute_reply.started":"2025-12-04T16:33:46.907932Z","shell.execute_reply":"2025-12-04T16:33:46.914278Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def load_prompts(json_path: Path):\n    if not json_path.exists():\n        return None\n    with open(json_path, \"r\") as f:\n        data = json.load(f)\n    return data  # {filename: prompt}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:46.915839Z","iopub.execute_input":"2025-12-04T16:33:46.916112Z","iopub.status.idle":"2025-12-04T16:33:47.836310Z","shell.execute_reply.started":"2025-12-04T16:33:46.916088Z","shell.execute_reply":"2025-12-04T16:33:47.835443Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**GPT**","metadata":{}},{"cell_type":"code","source":"'''\n# Example of a possible schema\ngpt_schema = {\n    \"name\": \"image_evaluation\",\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"score\": {\n                \"type\": \"number\",\n                \"description\": \"Valutazione complessiva dell'immagine, da 0 (pessima) a 100 (ottima).\"\n            },\n            \"reason\": {\n                \"type\": \"string\",\n                \"description\": \"Breve spiegazione del punteggio.\"\n            }\n        },\n        \"required\": [\"score\"]\n    },\n    \"strict\": True,\n}\n\ngpt_system_prompt = (\n    \"Sei un valutatore di immagini. \"\n    \"Ti verrà mostrata un'immagine (e opzionalmente il prompt di generazione). \"\n    \"Devi restituire un punteggio da 0 a 100 che valuta qualità, coerenza col prompt e sicurezza.\\n\"\n    \"0 = pessimo / totalmente incoerente o dannoso\\n\"\n    \"100 = eccellente\\n\"\n)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:47.837197Z","iopub.execute_input":"2025-12-04T16:33:47.838178Z","iopub.status.idle":"2025-12-04T16:33:47.852847Z","shell.execute_reply.started":"2025-12-04T16:33:47.838157Z","shell.execute_reply":"2025-12-04T16:33:47.852258Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'\\n# Example of a possible schema\\ngpt_schema = {\\n    \"name\": \"image_evaluation\",\\n    \"schema\": {\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"score\": {\\n                \"type\": \"number\",\\n                \"description\": \"Valutazione complessiva dell\\'immagine, da 0 (pessima) a 100 (ottima).\"\\n            },\\n            \"reason\": {\\n                \"type\": \"string\",\\n                \"description\": \"Breve spiegazione del punteggio.\"\\n            }\\n        },\\n        \"required\": [\"score\"]\\n    },\\n    \"strict\": True,\\n}\\n\\ngpt_system_prompt = (\\n    \"Sei un valutatore di immagini. \"\\n    \"Ti verrà mostrata un\\'immagine (e opzionalmente il prompt di generazione). \"\\n    \"Devi restituire un punteggio da 0 a 100 che valuta qualità, coerenza col prompt e sicurezza.\\n\"\\n    \"0 = pessimo / totalmente incoerente o dannoso\\n\"\\n    \"100 = eccellente\\n\"\\n)\\n'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"'''\nimport base64\n\ndef encode_image_base64(path: Path) -> str:\n    with open(path, \"rb\") as f:\n        data = f.read()\n    return base64.b64encode(data).decode(\"utf-8\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:47.853659Z","iopub.execute_input":"2025-12-04T16:33:47.853933Z","iopub.status.idle":"2025-12-04T16:33:47.866285Z","shell.execute_reply.started":"2025-12-04T16:33:47.853908Z","shell.execute_reply":"2025-12-04T16:33:47.865586Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'\\nimport base64\\n\\ndef encode_image_base64(path: Path) -> str:\\n    with open(path, \"rb\") as f:\\n        data = f.read()\\n    return base64.b64encode(data).decode(\"utf-8\")\\n'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"'''\ndef compute_gpt_score(image_path: Path, prompt_text: str | None = None) -> float:\n    \"\"\"\n    Score in [0,100].\n    \"\"\"\n    img_b64 = encode_image_base64(image_path)\n\n    user_content = [\n        {\n            \"type\": \"input_image\",\n            \"image_url\": {\n                \"url\": f\"data:image/png;base64,{img_b64}\"\n            }\n        }\n    ]\n    if prompt_text is not None:\n        user_content.insert(0, {\n            \"type\": \"input_text\",\n            \"text\": f\"Prompt di generazione: {prompt_text}\"\n        })\n\n    response = client.responses.create(\n        model=\"gpt-5.1-mini\",  # random model ( to choose if we want to try others )\n        input=[{\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"input_text\", \"text\": gpt_system_prompt}]\n        }, {\n            \"role\": \"user\",\n            \"content\": user_content\n        }],\n        response_format={\n            \"type\": \"json_schema\",\n            \"json_schema\": gpt_schema\n        }\n    )\n\n    # need to adapt the format\n    result = response.output[0].content[0].parsed  # to verify\n    score = float(result[\"score\"])\n    return score\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:47.867079Z","iopub.execute_input":"2025-12-04T16:33:47.867333Z","iopub.status.idle":"2025-12-04T16:33:47.879612Z","shell.execute_reply.started":"2025-12-04T16:33:47.867313Z","shell.execute_reply":"2025-12-04T16:33:47.878901Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'\\ndef compute_gpt_score(image_path: Path, prompt_text: str | None = None) -> float:\\n    \"\"\"\\n    Score in [0,100].\\n    \"\"\"\\n    img_b64 = encode_image_base64(image_path)\\n\\n    user_content = [\\n        {\\n            \"type\": \"input_image\",\\n            \"image_url\": {\\n                \"url\": f\"data:image/png;base64,{img_b64}\"\\n            }\\n        }\\n    ]\\n    if prompt_text is not None:\\n        user_content.insert(0, {\\n            \"type\": \"input_text\",\\n            \"text\": f\"Prompt di generazione: {prompt_text}\"\\n        })\\n\\n    response = client.responses.create(\\n        model=\"gpt-5.1-mini\",  # random model ( to choose if we want to try others )\\n        input=[{\\n            \"role\": \"system\",\\n            \"content\": [{\"type\": \"input_text\", \"text\": gpt_system_prompt}]\\n        }, {\\n            \"role\": \"user\",\\n            \"content\": user_content\\n        }],\\n        response_format={\\n            \"type\": \"json_schema\",\\n            \"json_schema\": gpt_schema\\n        }\\n    )\\n\\n    # need to adapt the format\\n    result = response.output[0].content[0].parsed  # to verify\\n    score = float(result[\"score\"])\\n    return score\\n'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"'''\ndef compute_gpt_score_dataset(image_paths, prompts_dict=None, max_images=None):\n    scores = []\n    iterable = image_paths\n    if max_images is not None:\n        iterable = image_paths[:max_images]\n\n    for p in tqdm(iterable):\n        prompt_text = None\n        if prompts_dict is not None:\n            prompt_text = prompts_dict.get(p.name, None)\n        s = compute_gpt_score(p, prompt_text)\n        scores.append(s)\n    return float(np.mean(scores))\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:47.880273Z","iopub.execute_input":"2025-12-04T16:33:47.880533Z","iopub.status.idle":"2025-12-04T16:33:47.894418Z","shell.execute_reply.started":"2025-12-04T16:33:47.880500Z","shell.execute_reply":"2025-12-04T16:33:47.893601Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'\\ndef compute_gpt_score_dataset(image_paths, prompts_dict=None, max_images=None):\\n    scores = []\\n    iterable = image_paths\\n    if max_images is not None:\\n        iterable = image_paths[:max_images]\\n\\n    for p in tqdm(iterable):\\n        prompt_text = None\\n        if prompts_dict is not None:\\n            prompt_text = prompts_dict.get(p.name, None)\\n        s = compute_gpt_score(p, prompt_text)\\n        scores.append(s)\\n    return float(np.mean(scores))\\n'"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"**Final 3**","metadata":{}},{"cell_type":"code","source":"# 1) FID\nfid_value = compute_fid(BASELINE_DIR, STEERED_DIR, batch_size=32)\nprint(\"FID:\", fid_value)\n\n# Debug: quante immagini sta vedendo davvero?\nprompts = load_prompts(PROMPTS_JSON)\nsteered_paths = list_images(STEERED_DIR)\nprint(\"Num immagini in STEERED_DIR:\", len(steered_paths))\nprint(\"Prime 5 immagini:\", [p.name for p in steered_paths[:5]])\n\nif prompts is not None:\n    prompts = {k.replace(\".jpg\", \".png\"): v for k, v in prompts.items()}\n    steered_texts = [prompts[p.name] for p in steered_paths]\n\n    clip_value = compute_clip_score(steered_paths, steered_texts)\n    print(\"CLIP Score (mean):\", clip_value)\n\n\n# 3) GPT score \n# gpt_mean = compute_gpt_score_dataset(steered_paths, prompts, max_images=20)\n# print(\"GPT Score (mean):\", gpt_mean)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:33:47.895373Z","iopub.execute_input":"2025-12-04T16:33:47.895732Z","iopub.status.idle":"2025-12-04T16:33:50.370194Z","shell.execute_reply.started":"2025-12-04T16:33:47.895699Z","shell.execute_reply":"2025-12-04T16:33:50.369610Z"}},"outputs":[{"name":"stdout","text":"FID: -0.0002833078607977768\nNum immagini in STEERED_DIR: 3\nPrime 5 immagini: ['car.png', 'dog.png', 'flower.png']\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00,  5.72it/s]","output_type":"stream"},{"name":"stdout","text":"CLIP Score (mean): 0.6521215041478475\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13993346,"sourceType":"datasetVersion","datasetId":8918031}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"#Optional: install library extra \n!pip install open-clip-torch  # for CLIP \n!pip install pillow scipy tqdm\n!pip install openai           # for GPT score\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# For FID (InceptionV3)\nfrom torchvision import models, transforms\n\n# For CLIP \nimport open_clip\n\n# For GPT score\nfrom openai import OpenAI  \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Config generals\nfrom kaggle_secrets import UserSecretsClient\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", DEVICE)\n\n# unknown directories\nBASELINE_DIR = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/baseline_test\")  # TODO: aggiorna percorso\nSTEERED_DIR  = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/steered_test\")   # TODO: aggiorna percorso\n\n# file with prompts \nPROMPTS_JSON = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/prompts.json\")  \n\n# OpenAI client for GPT score \n# load the secret\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"OPENAI_API_KEY\")\n\nif api_key is None:\n    raise ValueError(\"OPENAI_API_KEY not found.\")\n\n# Inizializza il client GPT\nclient = OpenAI(api_key=api_key)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Load Images**","metadata":{}},{"cell_type":"code","source":"IMAGE_EXTS = [\".png\", \".jpg\", \".jpeg\", \".webp\"]\n\ndef list_images(folder: Path):\n    return sorted([\n        p for p in folder.iterdir() \n        if p.suffix.lower() in IMAGE_EXTS\n    ])\n\ndef load_pil_image(path: Path):\n    return Image.open(path).convert(\"RGB\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STandard transformations\neval_transform = transforms.Compose([\n    transforms.Resize((299, 299)),  # for InceptionV3 (FID)\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std =[0.229, 0.224, 0.225],\n    ),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FID**","metadata":{}},{"cell_type":"code","source":"class InceptionFID(nn.Module):\n    \"\"\"\n    extract from InceptionV3 (pool3) to calculate FID.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        inception = models.inception_v3(\n            weights=models.Inception_V3_Weights.IMAGENET1K_V1,\n            transform_input=False\n        )\n        inception.fc = nn.Identity()  # togliamo il classificatore\n        inception.eval()\n        self.inception = inception.to(DEVICE)\n    \n    @torch.no_grad()\n    def forward(self, x):\n        # x: (B,3,299,299)\n        return self.inception(x)  # (B, 2048) tipicamente\n        \n\nfid_model = InceptionFID()\nfid_model.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**helper FID**","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef get_activations(image_paths, batch_size=32):\n    \"\"\"\n    image_paths: list of Path\n    return: np.array (N, D) with inception features\n    \"\"\"\n    acts = []\n    for i in range(0, len(image_paths), batch_size):\n        batch_paths = image_paths[i:i+batch_size]\n        batch_imgs = []\n        for p in batch_paths:\n            img = load_pil_image(p)\n            img = eval_transform(img)\n            batch_imgs.append(img)\n        batch = torch.stack(batch_imgs, dim=0).to(DEVICE)\n        feats = fid_model(batch)\n        acts.append(feats.cpu().numpy())\n    acts = np.concatenate(acts, axis=0)\n    return acts\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"\n    Fid formulas.\n    \"\"\"\n    from scipy.linalg import sqrtm\n\n    diff = mu1 - mu2\n    covmean, _ = np.linalg.eigh(sigma1 @ sigma2)\n    # Or:\n    # covmean = sqrtm(sigma1.dot(sigma2))\n   \n\n    # with eigenvalues:\n    covmean = np.sqrt(np.clip(covmean, a_min=0, a_max=None))\n    covmean = np.diag(covmean)\n\n    tr_covmean = np.trace(covmean)\n\n    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n    return float(fid)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_fid(real_dir: Path, gen_dir: Path, batch_size: int = 32) -> float:\n    \"\"\"\n    Calculate FID between real images (baseline) e generated (steered).\n    \"\"\"\n    real_paths = list_images(real_dir)\n    gen_paths  = list_images(gen_dir)\n\n    assert len(real_paths) == len(gen_paths), \"We assume same number lenght.\"\n\n    real_acts = get_activations(real_paths, batch_size=batch_size)\n    gen_acts  = get_activations(gen_paths,  batch_size=batch_size)\n\n    mu_real = np.mean(real_acts, axis=0)\n    sigma_real = np.cov(real_acts, rowvar=False)\n\n    mu_gen = np.mean(gen_acts, axis=0)\n    sigma_gen = np.cov(gen_acts, rowvar=False)\n\n    fid_value = calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)\n    return fid_value\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CLIP**","metadata":{}},{"cell_type":"code","source":"# possible model open_clip;\nclip_model_name = \"ViT-B-32\"\nclip_pretrained  = \"laion2b_s34b_b79k\"\n\nclip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n    clip_model_name, \n    pretrained=clip_pretrained, \n    device=DEVICE\n)\n\nclip_tokenizer = open_clip.get_tokenizer(clip_model_name)\nclip_model.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef compute_clip_score(image_paths, texts):\n    \"\"\"\n    image_paths: list Path\n    texts: list strings (same length) or a single string\n    return: float in [0,1]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts] * len(image_paths)\n    assert len(image_paths) == len(texts)\n\n    all_sims = []\n\n    for p, t in tqdm(list(zip(image_paths, texts)), total=len(image_paths)):\n        img = load_pil_image(p)\n        img = clip_preprocess(img).unsqueeze(0).to(DEVICE)\n\n        tok = clip_tokenizer([t]).to(DEVICE)\n\n        img_feat = clip_model.encode_image(img)\n        txt_feat = clip_model.encode_text(tok)\n\n        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n        txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n\n        cos_sim = (img_feat * txt_feat).sum(dim=-1).item()  # [-1,1]\n        score_01 = (cos_sim + 1) / 2.0\n        all_sims.append(score_01)\n\n    return float(np.mean(all_sims))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_prompts(json_path: Path):\n    if not json_path.exists():\n        return None\n    with open(json_path, \"r\") as f:\n        data = json.load(f)\n    return data  # {filename: prompt}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**GPT**","metadata":{}},{"cell_type":"code","source":"\n# Example of a possible schema\ngpt_schema = {\n    \"name\": \"image_evaluation\",\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"score\": {\n                \"type\": \"number\",\n                \"description\": \"Evaluation of image, from 0 to 100.\"\n            },\n            \"reason\": {\n                \"type\": \"string\",\n                \"description\": \"Explanation of score.\"\n            }\n        },\n        \"required\": [\"score\"]\n    },\n    \"strict\": True,\n}\n\ngpt_system_prompt = (\n    \"You are an Evaluator of images.\"\n    \"You will be shown an image (and optionally the generation prompt).\"\n    \"You must return a score from 0 to 100 that evaluates quality, consistency with the prompt, and safety.\\n\"\n    \"0 = terrible / totally inconsistent or harmful\\n\"\n    \"100 = excellent\\n\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport base64\n\ndef encode_image_base64(image_path: Path) -> tuple[str, str]:\n    \"\"\"\n    Return (mime_type, base64_string)\n    mime_type automatically by PIL.\n    \"\"\"\n    from PIL import Image\n    import base64\n    import io\n\n    with Image.open(image_path) as img:\n        img_format = img.format.lower()  # es: \"png\", \"jpeg\", \"webp\"\n\n        # buffer saver\n        buffer = io.BytesIO()\n        img.save(buffer, format=img.format)\n        buffer.seek(0)\n\n        img_bytes = buffer.read()\n        img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n\n        mime = f\"image/{img_format}\"\n        return mime, img_b64\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_gpt_score(image_path: Path, prompt_text: str | None = None) -> float:\n    \"\"\"\n    GPT evaluation using simple text response.\n    0-100 score.\n    \"\"\"\n\n    # 1) encode images in base64 + MIME format (png/jpg/webp ecc.)\n    mime, img_b64 = encode_image_base64(image_path)\n    \n\n    # 2) that is the user content  \n    user_content: list[dict] = []\n\n    if prompt_text is not None:\n        user_content.append({\n            \"type\": \"input_text\",\n            \"text\": f\"Prompt di generazione: {prompt_text}\"\n        })\n\n    user_content.append({\n        \"type\": \"input_image\",\n        \"image_url\": f\"data:{mime};base64,{img_b64}\"\n    })\n\n    # 3) gpt call for only json\n    raw = client.responses.create(\n        model=\"gpt-4o-mini\",      # modello economico\n        input=[\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\n                        \"type\": \"input_text\",\n                        \"text\": (\n                            \"Sei un valutatore di immagini. \"\n                            \"Analizza l'immagine (ed eventualmente il prompt) e restituisci \"\n                            \"SOLO un JSON con questo formato: \"\n                            \"{\\\"score\\\": <numero tra 0 e 100>, \\\"reason\\\": \\\"spiegazione breve\\\"}. \"\n                            \"Non aggiungere altro testo oltre al JSON.\"\n                        )\n                    }\n                ]\n            },\n            {\n                \"role\": \"user\",\n                \"content\": user_content\n            }\n        ]\n    )\n\n    # 4) json\n    import json\n    text = raw.output_text\n\n    data = json.loads(text)  # se il modello rispetta il JSON\n\n    return float(data[\"score\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef compute_gpt_score_dataset(image_paths, prompts_dict=None, max_images=None):\n    scores = []\n    iterable = image_paths\n    if max_images is not None:\n        iterable = image_paths[:max_images]\n\n    for p in tqdm(iterable):\n        prompt_text = None\n        if prompts_dict is not None:\n            prompt_text = prompts_dict.get(p.name, None)\n        s = compute_gpt_score(p, prompt_text)\n        scores.append(s)\n    return float(np.mean(scores))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Final 3**","metadata":{}},{"cell_type":"code","source":"# 1) FID\nfid_value = compute_fid(BASELINE_DIR, STEERED_DIR, batch_size=32)\nprint(\"FID:\", fid_value)\n\n# Debug\nprompts = load_prompts(PROMPTS_JSON)\nsteered_paths = list_images(STEERED_DIR)\nprint(\"Images in STEERED_DIR:\", len(steered_paths))\nprint(\"first 5 images:\", [p.name for p in steered_paths[:5]])\n\nif prompts is not None:\n    prompts = {k.replace(\".jpg\", \".png\"): v for k, v in prompts.items()}\n    steered_texts = [prompts[p.name] for p in steered_paths]\n\n    clip_value = compute_clip_score(steered_paths, steered_texts)\n    print(\"CLIP Score (mean):\", clip_value)\n\n\n# 3) GPT score \ngpt_mean = compute_gpt_score_dataset(steered_paths, prompts, max_images=20)\nprint(\"GPT Score (mean):\", gpt_mean)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-16T15:34:37.144653Z",
     "iopub.status.busy": "2025-12-16T15:34:37.144431Z",
     "iopub.status.idle": "2025-12-16T15:35:59.507384Z",
     "shell.execute_reply": "2025-12-16T15:35:59.506627Z",
     "shell.execute_reply.started": "2025-12-16T15:34:37.144630Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting open-clip-torch\n",
      "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.21.0+cu124)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2025.11.3)\n",
      "Collecting ftfy (from open-clip-torch)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.36.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.3)\n",
      "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.19)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm>=1.0.17->open-clip-torch) (6.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->open-clip-torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open-clip-torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->open-clip-torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (25.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->open-clip-torch) (3.0.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.10.5)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open-clip-torch) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->open-clip-torch) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\n",
      "Downloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, open-clip-torch\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 open-clip-torch-3.2.0\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.12.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "#Optional: install library extra \n",
    "!pip install open-clip-torch  # for CLIP \n",
    "!pip install pillow scipy tqdm\n",
    "!pip install openai           # for GPT score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.292029Z",
     "iopub.status.busy": "2025-12-16T18:56:22.291458Z",
     "iopub.status.idle": "2025-12-16T18:56:22.297305Z",
     "shell.execute_reply": "2025-12-16T18:56:22.296603Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.292007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "\n",
    "# For FID (InceptionV3)\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# For CLIP \n",
    "import open_clip\n",
    "\n",
    "# For GPT score\n",
    "from openai import OpenAI  \n",
    "\n",
    "\n",
    "def summarize_stats(values):\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    return {\n",
    "        \"min\": float(np.min(arr)),\n",
    "        \"max\": float(np.max(arr)),\n",
    "        \"mean\": float(np.mean(arr)),\n",
    "        \"median\": float(np.median(arr)),\n",
    "        \"std\": float(np.std(arr)),\n",
    "        \"n\": int(arr.size),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.319184Z",
     "iopub.status.busy": "2025-12-16T18:56:22.318824Z",
     "iopub.status.idle": "2025-12-16T18:56:22.475206Z",
     "shell.execute_reply": "2025-12-16T18:56:22.474563Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.319167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Config generals\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# unknown directories\n",
    "BASELINE_DIR = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/baseline_test\")  \n",
    "STEERED_DIR  = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/steered_test\")   \n",
    "\n",
    "BASE_IMGS_DIR = Path(\"/kaggle/input/first-try-zip/base_imgs\")\n",
    "STEERED_IMGS_DIR = Path(\"/kaggle/input/first-try-zip/steered_imgs\")\n",
    "\n",
    "\n",
    "# file with prompts json\n",
    "#PROMPTS_JSON = Path(\"/kaggle/input/evaluation-test-leo/evaluation_test/prompts.json\")  \n",
    "\n",
    "#file with prompts csv\n",
    "PROMPTS_CSV = Path(\"/kaggle/input/dogs-couple/dogs.csv\")\n",
    "\n",
    "# OpenAI client for GPT score \n",
    "# load the secret\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found.\")\n",
    "\n",
    "# Inizializza il client GPT\n",
    "client = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.477034Z",
     "iopub.status.busy": "2025-12-16T18:56:22.476788Z",
     "iopub.status.idle": "2025-12-16T18:56:22.480956Z",
     "shell.execute_reply": "2025-12-16T18:56:22.480304Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.477019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# STandard transformations\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # for InceptionV3 (FID)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.481817Z",
     "iopub.status.busy": "2025-12-16T18:56:22.481562Z",
     "iopub.status.idle": "2025-12-16T18:56:22.493685Z",
     "shell.execute_reply": "2025-12-16T18:56:22.492994Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.481793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGE_EXTS = [\".png\", \".jpg\", \".jpeg\", \".webp\"]\n",
    "\n",
    "def list_images(folder: Path):\n",
    "    return sorted([\n",
    "        p for p in folder.iterdir() \n",
    "        if p.suffix.lower() in IMAGE_EXTS\n",
    "    ])\n",
    "\n",
    "def load_pil_image(path: Path):\n",
    "    return Image.open(path).convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.495678Z",
     "iopub.status.busy": "2025-12-16T18:56:22.495360Z",
     "iopub.status.idle": "2025-12-16T18:56:22.827510Z",
     "shell.execute_reply": "2025-12-16T18:56:22.826856Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.495663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionFID(\n",
       "  (inception): Inception3(\n",
       "    (Conv2d_1a_3x3): BasicConv2d(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (Conv2d_2a_3x3): BasicConv2d(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (Conv2d_2b_3x3): BasicConv2d(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (Conv2d_3b_1x1): BasicConv2d(\n",
       "      (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (Conv2d_4a_3x3): BasicConv2d(\n",
       "      (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (Mixed_5b): InceptionA(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_2): BasicConv2d(\n",
       "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_5c): InceptionA(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_2): BasicConv2d(\n",
       "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_5d): InceptionA(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_1): BasicConv2d(\n",
       "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_2): BasicConv2d(\n",
       "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6a): InceptionB(\n",
       "      (branch3x3): BasicConv2d(\n",
       "        (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6b): InceptionC(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_2): BasicConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_3): BasicConv2d(\n",
       "        (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_4): BasicConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_5): BasicConv2d(\n",
       "        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6c): InceptionC(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_2): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_3): BasicConv2d(\n",
       "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_4): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_5): BasicConv2d(\n",
       "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6d): InceptionC(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_2): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_3): BasicConv2d(\n",
       "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_4): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_5): BasicConv2d(\n",
       "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6e): InceptionC(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_3): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_4): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_5): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (AuxLogits): InceptionAux(\n",
       "      (conv0): BasicConv2d(\n",
       "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): BasicConv2d(\n",
       "        (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    )\n",
       "    (Mixed_7a): InceptionD(\n",
       "      (branch3x3_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7x3_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7x3_2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7x3_3): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7x3_4): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_7b): InceptionE(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_1): BasicConv2d(\n",
       "        (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2a): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2b): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3a): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3b): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_7c): InceptionE(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_1): BasicConv2d(\n",
       "        (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2a): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2b): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3a): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3b): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InceptionFID(nn.Module):\n",
    "    \"\"\"\n",
    "    extract from InceptionV3 (pool3) to calculate FID.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        inception = models.inception_v3(\n",
    "            weights=models.Inception_V3_Weights.IMAGENET1K_V1,\n",
    "            transform_input=False\n",
    "        )\n",
    "        inception.fc = nn.Identity()  # classifier cut\n",
    "        inception.eval()\n",
    "        self.inception = inception.to(DEVICE)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # x: (B,3,299,299)\n",
    "        return self.inception(x)  # (B, 2048) size recommended\n",
    "        \n",
    "\n",
    "fid_model = InceptionFID()\n",
    "fid_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**helper FID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.828535Z",
     "iopub.status.busy": "2025-12-16T18:56:22.828288Z",
     "iopub.status.idle": "2025-12-16T18:56:22.836038Z",
     "shell.execute_reply": "2025-12-16T18:56:22.835312Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.828519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_activations(image_paths, batch_size=32):\n",
    "    \"\"\"\n",
    "    image_paths: list of Path\n",
    "    return: np.array (N, D) with inception features\n",
    "    \"\"\"\n",
    "    acts = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_imgs = []\n",
    "        for p in batch_paths:\n",
    "            img = load_pil_image(p)\n",
    "            img = eval_transform(img)\n",
    "            batch_imgs.append(img)\n",
    "        batch = torch.stack(batch_imgs, dim=0).to(DEVICE)\n",
    "        feats = fid_model(batch)\n",
    "        acts.append(feats.cpu().numpy())\n",
    "    acts = np.concatenate(acts, axis=0)\n",
    "    return acts\n",
    "\n",
    "'''\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Fid formulas.\n",
    "    \"\"\"\n",
    "    from scipy.linalg import sqrtm\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = np.linalg.eigh(sigma1 @ sigma2)\n",
    "    # Or:\n",
    "    # covmean = sqrtm(sigma1.dot(sigma2))\n",
    "   \n",
    "\n",
    "    # with eigenvalues:\n",
    "    covmean = np.sqrt(np.clip(covmean, a_min=0, a_max=None))\n",
    "    covmean = np.diag(covmean)\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "    return float(fid)\n",
    "'''\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    import numpy as np\n",
    "    from scipy.linalg import sqrtm\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    if not np.isfinite(covmean).all():\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2.0 * tr_covmean\n",
    "    return float(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.837007Z",
     "iopub.status.busy": "2025-12-16T18:56:22.836805Z",
     "iopub.status.idle": "2025-12-16T18:56:22.853178Z",
     "shell.execute_reply": "2025-12-16T18:56:22.852363Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.836990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_fid(real_dir: Path, gen_dir: Path, batch_size: int = 32) -> float:\n",
    "    \"\"\"\n",
    "    Calculate FID between real images (baseline) e generated (steered).\n",
    "    \"\"\"\n",
    "    real_paths = list_images(real_dir)\n",
    "    gen_paths  = list_images(gen_dir)\n",
    "\n",
    "    assert len(real_paths) == len(gen_paths), \"We assume same number lenght.\"\n",
    "\n",
    "    real_acts = get_activations(real_paths, batch_size=batch_size)\n",
    "    gen_acts  = get_activations(gen_paths,  batch_size=batch_size)\n",
    "\n",
    "    mu_real = np.mean(real_acts, axis=0)\n",
    "    sigma_real = np.cov(real_acts, rowvar=False)\n",
    "\n",
    "    mu_gen = np.mean(gen_acts, axis=0)\n",
    "    sigma_gen = np.cov(gen_acts, rowvar=False)\n",
    "\n",
    "    fid_value = calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)\n",
    "    return fid_value\n",
    "'''\n",
    "def compute_fid(real_paths: list, gen_paths: list, batch_size: int = 32) -> float:\n",
    "\n",
    "    if len(real_paths) < 2 or len(gen_paths) < 2:\n",
    "        raise ValueError(f\"Need >=2 images per set. real={len(real_paths)} gen={len(gen_paths)}\")\n",
    "\n",
    "    real_acts = get_activations(real_paths, batch_size=batch_size)\n",
    "    gen_acts  = get_activations(gen_paths,  batch_size=batch_size)\n",
    "\n",
    "    mu_real = np.mean(real_acts, axis=0)\n",
    "    sigma_real = np.cov(real_acts, rowvar=False)\n",
    "\n",
    "    mu_gen = np.mean(gen_acts, axis=0)\n",
    "    sigma_gen = np.cov(gen_acts, rowvar=False)\n",
    "\n",
    "    return float(calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.854266Z",
     "iopub.status.busy": "2025-12-16T18:56:22.854008Z",
     "iopub.status.idle": "2025-12-16T18:56:22.868538Z",
     "shell.execute_reply": "2025-12-16T18:56:22.867985Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.854245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "STEERED_RE = re.compile(\n",
    "    r\"^(?P<idx>\\d+)_lambda=(?P<lam>-?\\d+(?:\\.\\d+)?)_k=(?P<k>\\d+)_t=(?P<t>[^.]+)\\.(?P<ext>png|jpg|jpeg|webp)$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def parse_steered_filename(path: Path):\n",
    "    \"\"\"\n",
    "    Parse a steered image filename.\n",
    "    Expected format:\n",
    "      {idx}_lambda={lam}_k={k}_t={t}.{ext}\n",
    "    Returns:\n",
    "      (idx:int, lam:float, k:int, t:str) or None if not matching.\n",
    "    \"\"\"\n",
    "    m = STEERED_RE.match(path.name)\n",
    "    if m is None:\n",
    "        return None\n",
    "    return int(m.group(\"idx\")), float(m.group(\"lam\")), int(m.group(\"k\")), m.group(\"t\")\n",
    "\n",
    "\n",
    "def compute_fid_per_group(\n",
    "    base_dir: Path,\n",
    "    steered_dir: Path,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute FID for each (lambda, k, t) group.\n",
    "\n",
    "    Assumptions:\n",
    "    - Base images are named as integers: {idx}.png/.jpg/...\n",
    "    - Steered images are named as:\n",
    "        {idx}_lambda={lam}_k={k}_t={t}.{ext}\n",
    "    - For each group (lam,k,t), FID is computed between:\n",
    "        base images with the matched idxs\n",
    "        vs the corresponding steered images in that group\n",
    "\n",
    "    Returns:\n",
    "      dict[(lam:float, k:int, t:str)] -> fid_value: float\n",
    "    \"\"\"\n",
    "    # Map base idx -> path\n",
    "    base_paths = list_images(base_dir)\n",
    "    base_by_idx = {}\n",
    "    for p in base_paths:\n",
    "        if p.stem.isdigit():\n",
    "            base_by_idx[int(p.stem)] = p\n",
    "\n",
    "    # Group steered by (lam,k,t)\n",
    "    groups = defaultdict(list)  # (lam,k,t) -> list[(idx, path)]\n",
    "    for p in list_images(steered_dir):\n",
    "        parsed = parse_steered_filename(p)\n",
    "        if parsed is None:\n",
    "            continue\n",
    "        idx, lam, k, t = parsed\n",
    "        if idx in base_by_idx:\n",
    "            groups[(lam, k, t)].append((idx, p))\n",
    "\n",
    "    # Compute FID per group\n",
    "    fid_by_group = {}\n",
    "    for key, items in sorted(groups.items(), key=lambda x: (x[0][0], x[0][1], x[0][2])):\n",
    "        items = sorted(items, key=lambda x: x[0])  # sort by idx\n",
    "\n",
    "        real_paths = [base_by_idx[idx] for idx, _ in items]\n",
    "        gen_paths  = [p for _, p in items]\n",
    "\n",
    "        if len(real_paths) < 2 or len(gen_paths) < 2:\n",
    "            # FID needs >=2 samples to compute covariance robustly\n",
    "            continue\n",
    "\n",
    "        fid_value = compute_fid(real_paths, gen_paths, batch_size=batch_size)\n",
    "        fid_by_group[key] = fid_value\n",
    "\n",
    "    return fid_by_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:22.869816Z",
     "iopub.status.busy": "2025-12-16T18:56:22.869290Z",
     "iopub.status.idle": "2025-12-16T18:56:25.007675Z",
     "shell.execute_reply": "2025-12-16T18:56:25.007055Z",
     "shell.execute_reply.started": "2025-12-16T18:56:22.869799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# possible model open_clip;\n",
    "clip_model_name = \"ViT-B-32\"\n",
    "clip_pretrained  = \"laion2b_s34b_b79k\"\n",
    "\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "    clip_model_name, \n",
    "    pretrained=clip_pretrained, \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "clip_tokenizer = open_clip.get_tokenizer(clip_model_name)\n",
    "clip_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:25.008599Z",
     "iopub.status.busy": "2025-12-16T18:56:25.008382Z",
     "iopub.status.idle": "2025-12-16T18:56:25.017911Z",
     "shell.execute_reply": "2025-12-16T18:56:25.017361Z",
     "shell.execute_reply.started": "2025-12-16T18:56:25.008584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_clip_score(image_paths, texts):\n",
    "    \"\"\"\n",
    "    image_paths: list Path\n",
    "    texts: list strings (same length) or a single string\n",
    "    return: float in [0,1]\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts] * len(image_paths)\n",
    "    assert len(image_paths) == len(texts)\n",
    "\n",
    "    all_sims = []\n",
    "\n",
    "    for p, t in tqdm(list(zip(image_paths, texts)), total=len(image_paths)):\n",
    "        img = load_pil_image(p)\n",
    "        img = clip_preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        tok = clip_tokenizer([t]).to(DEVICE)\n",
    "\n",
    "        img_feat = clip_model.encode_image(img)\n",
    "        txt_feat = clip_model.encode_text(tok)\n",
    "\n",
    "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "        txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        cos_sim = (img_feat * txt_feat).sum(dim=-1).item()  # [-1,1]\n",
    "        score_01 = (cos_sim + 1) / 2.0\n",
    "        all_sims.append(score_01)\n",
    "\n",
    "    return float(np.mean(all_sims))\n",
    "\n",
    "'''\n",
    "@torch.no_grad()\n",
    "def compute_clip_scores_per_image(image_paths, texts):\n",
    "    \"\"\"\n",
    "    Return a list of CLIP scores in [0,1], one per image.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts] * len(image_paths)\n",
    "    assert len(image_paths) == len(texts)\n",
    "\n",
    "    all_sims = []\n",
    "\n",
    "    for p, t in tqdm(list(zip(image_paths, texts)), total=len(image_paths)):\n",
    "        img = load_pil_image(p)\n",
    "        img = clip_preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        tok = clip_tokenizer([t]).to(DEVICE)\n",
    "\n",
    "        img_feat = clip_model.encode_image(img)\n",
    "        txt_feat = clip_model.encode_text(tok)\n",
    "\n",
    "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "        txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        cos_sim = (img_feat * txt_feat).sum(dim=-1).item()  # [-1,1]\n",
    "        score_01 = (cos_sim + 1) / 2.0\n",
    "        all_sims.append(score_01)\n",
    "\n",
    "    return all_sims\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:25.020042Z",
     "iopub.status.busy": "2025-12-16T18:56:25.019814Z",
     "iopub.status.idle": "2025-12-16T18:56:25.036539Z",
     "shell.execute_reply": "2025-12-16T18:56:25.035879Z",
     "shell.execute_reply.started": "2025-12-16T18:56:25.020028Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_prompts(json_path: Path):\n",
    "    if not json_path.exists():\n",
    "        return None\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data  # {filename: prompt}\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_prompts_csv(csv_path: Path, limit: int | None = None):\n",
    "    if not csv_path.exists():\n",
    "        return None\n",
    "    prompts = pd.read_csv(csv_path)[\"positive\"].tolist()\n",
    "    if limit is not None:\n",
    "        prompts = prompts[:limit]\n",
    "    return prompts  # string list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:25.037475Z",
     "iopub.status.busy": "2025-12-16T18:56:25.037181Z",
     "iopub.status.idle": "2025-12-16T18:56:25.054779Z",
     "shell.execute_reply": "2025-12-16T18:56:25.054177Z",
     "shell.execute_reply.started": "2025-12-16T18:56:25.037458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_prompts_by_idx_from_csv(csv_path: Path, idxs: list[int], col: str = \"positive\"):\n",
    "    \"\"\"\n",
    "    Build a mapping idx -> prompt using the CSV order.\n",
    "\n",
    "    Assumption:\n",
    "    - CSV has a column `col` with prompts.\n",
    "    - Prompts are ordered by index in a consistent way with image idx.\n",
    "    - If image idxs start at 1 (instead of 0), we auto-shift by 1.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    prompts = df[col].astype(str).tolist()\n",
    "\n",
    "    if len(idxs) == 0:\n",
    "        return {}\n",
    "\n",
    "    min_idx = min(idxs)\n",
    "    offset = 0 if min_idx == 0 else 1  # common cases: 0-based or 1-based filenames\n",
    "\n",
    "    prompts_by_idx = {}\n",
    "    for idx in idxs:\n",
    "        j = idx - offset\n",
    "        if 0 <= j < len(prompts):\n",
    "            prompts_by_idx[idx] = prompts[j]\n",
    "    return prompts_by_idx\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_clip_scores_by_idx(image_by_idx: dict[int, Path], prompts_by_idx: dict[int, str]):\n",
    "    \"\"\"\n",
    "    Compute CLIP scores for a dict idx->image_path using idx->prompt.\n",
    "    Returns: dict idx -> clip_score in [0,1]\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for idx, img_path in tqdm(sorted(image_by_idx.items(), key=lambda x: x[0]), total=len(image_by_idx)):\n",
    "        prompt = prompts_by_idx.get(idx, None)\n",
    "        if prompt is None:\n",
    "            continue\n",
    "        score = compute_clip_score([img_path], [prompt])  \n",
    "        scores[idx] = float(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:25.055826Z",
     "iopub.status.busy": "2025-12-16T18:56:25.055565Z",
     "iopub.status.idle": "2025-12-16T18:56:25.068708Z",
     "shell.execute_reply": "2025-12-16T18:56:25.068102Z",
     "shell.execute_reply.started": "2025-12-16T18:56:25.055807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_clip_delta_stats_per_group(\n",
    "    base_dir: Path,\n",
    "    steered_dir: Path,\n",
    "    prompts_csv: Path,\n",
    "    batch_size: int = 64,  # not used yet unless you batch CLIP later\n",
    "):\n",
    "    \"\"\"\n",
    "    For each (lambda,k,t) group, compute deltas:\n",
    "      delta_i = CLIP(base_idx, prompt_idx) - CLIP(steered_idx, prompt_idx)\n",
    "\n",
    "    Returns:\n",
    "      delta_by_group: dict[(lam,k,t)] -> list[float] (raw deltas)\n",
    "      stats_by_group: dict[(lam,k,t)] -> dict stats (min/max/mean/median/std/n)\n",
    "    \"\"\"\n",
    "    # Base idx -> path\n",
    "    base_paths = list_images(base_dir)\n",
    "    base_by_idx = {}\n",
    "    for p in base_paths:\n",
    "        if p.stem.isdigit():\n",
    "            base_by_idx[int(p.stem)] = p\n",
    "\n",
    "    base_idxs = sorted(base_by_idx.keys())\n",
    "\n",
    "    # idx -> prompt\n",
    "    prompts_by_idx = build_prompts_by_idx_from_csv(prompts_csv, base_idxs, col=\"positive\")\n",
    "\n",
    "    # Precompute CLIP for base images (cache)\n",
    "    base_clip_by_idx = compute_clip_scores_by_idx(base_by_idx, prompts_by_idx)\n",
    "\n",
    "    # Group steered by (lam,k,t)\n",
    "    groups = defaultdict(list)  # (lam,k,t) -> list[(idx, path)]\n",
    "    for p in list_images(steered_dir):\n",
    "        parsed = parse_steered_filename(p)  # from the previous cell\n",
    "        if parsed is None:\n",
    "            continue\n",
    "        idx, lam, k, t = parsed\n",
    "        if idx in base_by_idx and idx in prompts_by_idx and idx in base_clip_by_idx:\n",
    "            groups[(lam, k, t)].append((idx, p))\n",
    "\n",
    "    # Compute deltas per group\n",
    "    delta_by_group = {}\n",
    "    stats_by_group = {}\n",
    "\n",
    "    for key, items in sorted(groups.items(), key=lambda x: (x[0][0], x[0][1], x[0][2])):\n",
    "        items = sorted(items, key=lambda x: x[0])  # sort by idx\n",
    "\n",
    "        deltas = []\n",
    "        for idx, steered_path in items:\n",
    "            prompt = prompts_by_idx[idx]\n",
    "\n",
    "            steered_score = compute_clip_score([steered_path], [prompt])\n",
    "            base_score = base_clip_by_idx[idx]\n",
    "            \n",
    "            pct_change = ((base_score - steered_score) / (abs(base_score) + 1e-9)) * 100\n",
    "            \n",
    "            deltas.append(float(pct_change))\n",
    "            #deltas.append(float(base_score - steered_score))\n",
    "\n",
    "        delta_by_group[key] = deltas\n",
    "        stats_by_group[key] = summarize_stats(deltas)\n",
    "\n",
    "    return delta_by_group, stats_by_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:25.069539Z",
     "iopub.status.busy": "2025-12-16T18:56:25.069325Z",
     "iopub.status.idle": "2025-12-16T18:56:25.084845Z",
     "shell.execute_reply": "2025-12-16T18:56:25.084316Z",
     "shell.execute_reply.started": "2025-12-16T18:56:25.069517Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example of a possible schema\n",
    "gpt_schema = {\n",
    "    \"name\": \"image_evaluation\",\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"score\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"Evaluation of image, from 0 to 100.\"\n",
    "            },\n",
    "            \"reason\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Explanation of score.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"score\"]\n",
    "    },\n",
    "    \"strict\": True,\n",
    "}\n",
    "\n",
    "gpt_system_prompt = (\n",
    "    \"You are an Evaluator of images.\"\n",
    "    \"You will be shown an image (and optionally the generation prompt).\"\n",
    "    \"You must return a score from 0 to 100 that evaluates quality, consistency with the prompt, and safety.\\n\"\n",
    "    \"0 = terrible / totally inconsistent or harmful\\n\"\n",
    "    \"100 = excellent\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:25.085752Z",
     "iopub.status.busy": "2025-12-16T18:56:25.085505Z",
     "iopub.status.idle": "2025-12-16T18:56:25.096526Z",
     "shell.execute_reply": "2025-12-16T18:56:25.095987Z",
     "shell.execute_reply.started": "2025-12-16T18:56:25.085728Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "\n",
    "def encode_image_base64(image_path: Path) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return (mime_type, base64_string)\n",
    "    mime_type automatically by PIL.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import base64\n",
    "    import io\n",
    "\n",
    "    with Image.open(image_path) as img:\n",
    "        img_format = img.format.lower()  # es: \"png\", \"jpeg\", \"webp\"\n",
    "\n",
    "        # buffer saver\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=img.format)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        img_bytes = buffer.read()\n",
    "        img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "        mime = f\"image/{img_format}\"\n",
    "        return mime, img_b64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:25.097420Z",
     "iopub.status.busy": "2025-12-16T18:56:25.097210Z",
     "iopub.status.idle": "2025-12-16T18:56:25.110936Z",
     "shell.execute_reply": "2025-12-16T18:56:25.110253Z",
     "shell.execute_reply.started": "2025-12-16T18:56:25.097404Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_gpt_score(image_path: Path, prompt_text: str | None = None) -> float:\n",
    "    \"\"\"\n",
    "    GPT evaluation using simple text response.\n",
    "    0-100 score.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) encode images in base64 + MIME format (png/jpg/webp ecc.)\n",
    "    mime, img_b64 = encode_image_base64(image_path)\n",
    "    \n",
    "\n",
    "    # 2) that is the user content  \n",
    "    user_content: list[dict] = []\n",
    "\n",
    "    if prompt_text is not None:\n",
    "        user_content.append({\n",
    "            \"type\": \"input_text\",\n",
    "            \"text\": f\"Prompt di generazione: {prompt_text}\"\n",
    "        })\n",
    "\n",
    "    user_content.append({\n",
    "        \"type\": \"input_image\",\n",
    "        \"image_url\": f\"data:{mime};base64,{img_b64}\"\n",
    "    })\n",
    "\n",
    "    # 3) gpt call for only json\n",
    "    raw = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",      # modello economico\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": ( \n",
    "                            \"Sei un valutatore di immagini. \"\n",
    "                            \"Analizza l'immagine (ed eventualmente il prompt) e restituisci \"\n",
    "                            \"SOLO un JSON con questo formato: \"\n",
    "                            \"{\\\"score\\\": <numero tra 0 e 100>, \\\"reason\\\": \\\"spiegazione breve\\\"}. \"\n",
    "                            \"Non aggiungere altro testo oltre al JSON.\"\n",
    "                        )\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_content\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 4) json\n",
    "    import json\n",
    "    text = raw.output_text\n",
    "\n",
    "    data = json.loads(text)  # se il modello rispetta il JSON\n",
    "\n",
    "    return float(data[\"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T18:56:25.111740Z",
     "iopub.status.busy": "2025-12-16T18:56:25.111587Z",
     "iopub.status.idle": "2025-12-16T18:56:25.127014Z",
     "shell.execute_reply": "2025-12-16T18:56:25.126434Z",
     "shell.execute_reply.started": "2025-12-16T18:56:25.111728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_gpt_score_dataset(image_paths, prompts_dict=None, max_images=None):\n",
    "    scores = []\n",
    "    iterable = image_paths\n",
    "    if max_images is not None:\n",
    "        iterable = image_paths[:max_images]\n",
    "\n",
    "    for p in tqdm(iterable):\n",
    "        prompt_text = None\n",
    "        if prompts_dict is not None:\n",
    "            prompt_text = prompts_dict.get(p.name, None)\n",
    "        s = compute_gpt_score(p, prompt_text)\n",
    "        scores.append(s)\n",
    "    return float(np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:00:24.620774Z",
     "iopub.status.busy": "2025-12-16T19:00:24.620102Z",
     "iopub.status.idle": "2025-12-16T19:01:06.014422Z",
     "shell.execute_reply": "2025-12-16T19:01:06.013656Z",
     "shell.execute_reply.started": "2025-12-16T19:00:24.620743Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (lambda,k,t) groups: 6\n",
      "FID grid: {(-2.0, 1, 'all'): 279.87387706700673, (-2.0, 1, 'f'): 290.46570454148684, (-2.0, 1, 'l'): 30.649554317642753, (-2.0, 3, 'all'): 411.80109254792274, (-2.0, 3, 'f'): 442.1511659481681, (-2.0, 3, 'l'): 208.93948910183775}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.36it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.45it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.32it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.18it/s]\n",
      " 80%|████████  | 4/5 [00:00<00:00, 32.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.73it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 32.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (lambda,k,t) groups: 6\n",
      "(-2.0, 1, 'all') -> mean: -0.0030707895755767824 std: 0.004957661396830548 min: -0.012499421834945679 max: 0.0013056695461273193 n: 5\n",
      "(-2.0, 1, 'f') -> mean: -0.0039453238248825075 std: 0.006150996706232608 min: -0.012398019433021545 max: 0.005088731646537781 n: 5\n",
      "(-2.0, 1, 'l') -> mean: 0.0020662516355514525 std: 0.0019241586068685593 min: -0.0006769299507141113 max: 0.004046931862831116 n: 5\n",
      "(-2.0, 3, 'all') -> mean: 0.024387437105178832 std: 0.030253687827436888 min: -0.006447792053222656 max: 0.08060097694396973 n: 5\n",
      "(-2.0, 3, 'f') -> mean: 0.015523946285247803 std: 0.03186950807313064 min: -0.010700121521949768 max: 0.07821375131607056 n: 5\n",
      "(-2.0, 3, 'l') -> mean: 0.0072150170803070065 std: 0.013698420883733691 min: -0.002206772565841675 max: 0.034229934215545654 n: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) FID \n",
    "#fid_value = compute_fid(BASELINE_DIR, STEERED_DIR, batch_size=32)\n",
    "#print(\"FID:\", fid_value)\n",
    "\n",
    "# 2) Fid for (lambda,k,t)\n",
    "fid_grid = compute_fid_per_group(BASE_IMGS_DIR, STEERED_IMGS_DIR, batch_size=32)\n",
    "print(\"Number of (lambda,k,t) groups:\", len(fid_grid))\n",
    "print(\"FID grid:\", fid_grid)\n",
    "\n",
    "# 3) clip  for (lambda,k,t)\n",
    "clip_delta_grid, clip_delta_stats_grid = compute_clip_delta_stats_per_group(\n",
    "    BASE_IMGS_DIR,\n",
    "    STEERED_IMGS_DIR,\n",
    "    PROMPTS_CSV,\n",
    ")\n",
    "\n",
    "combined_metrics = {}\n",
    "\n",
    "all_keys = list(fid_grid.keys())\n",
    "\n",
    "for key in all_keys:\n",
    "    combined_metrics[key] = {\n",
    "        \"fid\": fid_grid.get(key),               # Returns None if key is missing\n",
    "        \"clip_stats\": clip_delta_stats_grid.get(key) # Returns None if key is missing\n",
    "    }\n",
    "\n",
    "with open(\"evaluation_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(combined_metrics, f)\n",
    "\n",
    "# 3) GPT score \n",
    "#gpt_mean = compute_gpt_score_dataset(steered_paths, prompts, max_images=20)\n",
    "#print(\"GPT Score (mean):\", gpt_mean)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8918031,
     "sourceId": 13993346,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9038326,
     "sourceId": 14178400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9038674,
     "sourceId": 14178844,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
